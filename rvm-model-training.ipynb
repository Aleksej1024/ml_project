{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install easing-functions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:08:08.706596Z","iopub.execute_input":"2025-06-14T18:08:08.706966Z","iopub.status.idle":"2025-06-14T18:08:13.191467Z","shell.execute_reply.started":"2025-06-14T18:08:08.706938Z","shell.execute_reply":"2025-06-14T18:08:13.190494Z"}},"outputs":[{"name":"stdout","text":"Collecting easing-functions\n  Downloading easing_functions-1.0.4-py3-none-any.whl.metadata (1.6 kB)\nDownloading easing_functions-1.0.4-py3-none-any.whl (15 kB)\nInstalling collected packages: easing-functions\nSuccessfully installed easing-functions-1.0.4\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import gdown\nimport shutil\nimport tarfile\nimport argparse\nimport torch\nimport random\nimport os\nimport sys\nfrom torch import nn\nfrom torch import distributed as dist\nfrom torch import multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.optim import Adam\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.utils import make_grid\nfrom torchvision.transforms.functional import center_crop\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as FT\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom typing import Tuple, Optional, List\nfrom torch import Tensor\nfrom tqdm import tqdm\nfrom torchvision.models.mobilenetv3 import MobileNetV3, InvertedResidualConfig\nfrom torchvision.transforms.functional import normalize\nimport easing_functions as ef","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:08:13.193243Z","iopub.execute_input":"2025-06-14T18:08:13.193478Z","iopub.status.idle":"2025-06-14T18:08:38.544929Z","shell.execute_reply.started":"2025-06-14T18:08:13.193453Z","shell.execute_reply":"2025-06-14T18:08:38.544158Z"}},"outputs":[{"name":"stderr","text":"2025-06-14 18:08:21.528326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924501.756388      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924501.823307      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"url = 'https://drive.google.com/uc?id=1-S4F-rB75E8I7YUpHfu3itIl1knFhhFF'\noutput = './VideoMatte240K_JPEG_SD.tar'\ngdown.download(url, output, quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:08:38.545660Z","iopub.execute_input":"2025-06-14T18:08:38.546210Z","iopub.status.idle":"2025-06-14T18:09:35.012193Z","shell.execute_reply.started":"2025-06-14T18:08:38.546190Z","shell.execute_reply":"2025-06-14T18:09:35.011506Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom (original): https://drive.google.com/uc?id=1-S4F-rB75E8I7YUpHfu3itIl1knFhhFF\nFrom (redirected): https://drive.google.com/uc?id=1-S4F-rB75E8I7YUpHfu3itIl1knFhhFF&confirm=t&uuid=01be407a-3ac7-4b08-b601-79cd9eff7ae8\nTo: /kaggle/working/VideoMatte240K_JPEG_SD.tar\n100%|██████████| 6.11G/6.11G [00:53<00:00, 114MB/s] \n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'./VideoMatte240K_JPEG_SD.tar'"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"url = 'https://drive.google.com/uc?id=1FqD-HfwXwbeTswQEIFaQkaVWUh_i6cSy'\noutput = './Backgrounds_Validation.tar'\ngdown.download(url, output, quiet=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:09:35.014126Z","iopub.execute_input":"2025-06-14T18:09:35.014539Z","iopub.status.idle":"2025-06-14T18:09:40.405419Z","shell.execute_reply.started":"2025-06-14T18:09:35.014520Z","shell.execute_reply":"2025-06-14T18:09:40.404718Z"}},"outputs":[{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1FqD-HfwXwbeTswQEIFaQkaVWUh_i6cSy\nTo: /kaggle/working/Backgrounds_Validation.tar\n100%|██████████| 57.4M/57.4M [00:01<00:00, 46.6MB/s]\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'./Backgrounds_Validation.tar'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"with tarfile.open('./VideoMatte240K_JPEG_SD.tar', 'r') as zip_file:\n    zip_file.extractall('./')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:09:40.406230Z","iopub.execute_input":"2025-06-14T18:09:40.406507Z","iopub.status.idle":"2025-06-14T18:11:13.173226Z","shell.execute_reply.started":"2025-06-14T18:09:40.406482Z","shell.execute_reply":"2025-06-14T18:11:13.172656Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"train_path = './Backgrounds/train'\nvalid_path = './Backgrounds/valid'\n# Извлечение файлов из архива\nwith tarfile.open('./Backgrounds_Validation.tar', 'r') as tar_file:\n    tar_file.extractall('./ImageMatte')\n# Получение списка файлов\nfiles = os.listdir('./ImageMatte/Backgrounds')\n# Перемешивание файлов\nrandom.shuffle(files)\n# Определение количества файлов для обучения и валидации\ntrain_count = int(len(files) * 0.8)\ntrain_files = files[:train_count]\nvalid_files = files[train_count:]\n# Создание папок, если они не существуют\nos.makedirs(train_path, exist_ok=True)\nos.makedirs(valid_path, exist_ok=True)\n# Перемещение файлов в соответствующие папки\nfor file in train_files:\n    shutil.move(os.path.join('./ImageMatte/Backgrounds', file), os.path.join(train_path, file))\nfor file in valid_files:\n    shutil.move(os.path.join('./ImageMatte/Backgrounds', file), os.path.join(valid_path, file))\n# Удаление временной папки с извлеченными файлами\nshutil.rmtree('./ImageMatte')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.174021Z","iopub.execute_input":"2025-06-14T18:11:13.174296Z","iopub.status.idle":"2025-06-14T18:11:13.277723Z","shell.execute_reply.started":"2025-06-14T18:11:13.174273Z","shell.execute_reply":"2025-06-14T18:11:13.276941Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"DATA_PATHS = {\n\n    'videomatte': {\n        'train': './VideoMatte240K_JPEG_SD/train',\n        'valid': './VideoMatte240K_JPEG_SD/test',\n    },\n    'background_images': {\n            'train': './Backgrounds/train',\n            'valid': './Backgrounds/valid',\n    },\n\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.278562Z","iopub.execute_input":"2025-06-14T18:11:13.279033Z","iopub.status.idle":"2025-06-14T18:11:13.283233Z","shell.execute_reply.started":"2025-06-14T18:11:13.279007Z","shell.execute_reply":"2025-06-14T18:11:13.282580Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"\nclass MotionAugmentation:\n    def __init__(self,\n                 size,\n                 prob_fgr_affine,\n                 prob_bgr_affine,\n                 prob_noise,\n                 prob_color_jitter,\n                 prob_grayscale,\n                 prob_sharpness,\n                 prob_blur,\n                 prob_hflip,\n                 prob_pause,\n                 static_affine=True,\n                 aspect_ratio_range=(0.9, 1.1)):\n        self.size = size\n        self.prob_fgr_affine = prob_fgr_affine\n        self.prob_bgr_affine = prob_bgr_affine\n        self.prob_noise = prob_noise\n        self.prob_color_jitter = prob_color_jitter\n        self.prob_grayscale = prob_grayscale\n        self.prob_sharpness = prob_sharpness\n        self.prob_blur = prob_blur\n        self.prob_hflip = prob_hflip\n        self.prob_pause = prob_pause\n        self.static_affine = static_affine\n        self.aspect_ratio_range = aspect_ratio_range\n        \n    def __call__(self, fgrs, phas, bgrs):\n        # Foreground affine\n        if random.random() < self.prob_fgr_affine:\n            fgrs, phas = self._motion_affine(fgrs, phas)\n\n        # Background affine\n        if random.random() < self.prob_bgr_affine / 2:\n            bgrs = self._motion_affine(bgrs)\n        if random.random() < self.prob_bgr_affine / 2:\n            fgrs, phas, bgrs = self._motion_affine(fgrs, phas, bgrs)\n                \n        # Still Affine\n        if self.static_affine:\n            fgrs, phas = self._static_affine(fgrs, phas, scale_ranges=(0.5, 1))\n            bgrs = self._static_affine(bgrs, scale_ranges=(1, 1.5))\n        \n        # To tensor\n        fgrs = torch.stack([  FT.to_tensor(fgr) for fgr in fgrs])\n        phas = torch.stack([  FT.to_tensor(pha) for pha in phas])\n        bgrs = torch.stack([  FT.to_tensor(bgr) for bgr in bgrs])\n        \n        # Resize\n        params = transforms.RandomResizedCrop.get_params(fgrs, scale=(1, 1), ratio=self.aspect_ratio_range)\n        fgrs =   FT.resized_crop(fgrs, *params, self.size, interpolation=  FT.InterpolationMode.BILINEAR)\n        phas =   FT.resized_crop(phas, *params, self.size, interpolation=  FT.InterpolationMode.BILINEAR)\n        params = transforms.RandomResizedCrop.get_params(bgrs, scale=(1, 1), ratio=self.aspect_ratio_range)\n        bgrs =   FT.resized_crop(bgrs, *params, self.size, interpolation=  FT.InterpolationMode.BILINEAR)\n\n        # Horizontal flip\n        if random.random() < self.prob_hflip:\n            fgrs =   FT.hflip(fgrs)\n            phas =   FT.hflip(phas)\n        if random.random() < self.prob_hflip:\n            bgrs =   FT.hflip(bgrs)\n\n        # Noise\n        if random.random() < self.prob_noise:\n            fgrs, bgrs = self._motion_noise(fgrs, bgrs)\n        \n        # Color jitter\n        if random.random() < self.prob_color_jitter:\n            fgrs = self._motion_color_jitter(fgrs)\n        if random.random() < self.prob_color_jitter:\n            bgrs = self._motion_color_jitter(bgrs)\n            \n        # Grayscale\n        if random.random() < self.prob_grayscale:\n            fgrs =   FT.rgb_to_grayscale(fgrs, num_output_channels=3).contiguous()\n            bgrs =   FT.rgb_to_grayscale(bgrs, num_output_channels=3).contiguous()\n            \n        # Sharpen\n        if random.random() < self.prob_sharpness:\n            sharpness = random.random() * 8\n            fgrs =   FT.adjust_sharpness(fgrs, sharpness)\n            phas =   FT.adjust_sharpness(phas, sharpness)\n            bgrs =   FT.adjust_sharpness(bgrs, sharpness)\n        \n        # Blur\n        if random.random() < self.prob_blur / 3:\n            fgrs, phas = self._motion_blur(fgrs, phas)\n        if random.random() < self.prob_blur / 3:\n            bgrs = self._motion_blur(bgrs)\n        if random.random() < self.prob_blur / 3:\n            fgrs, phas, bgrs = self._motion_blur(fgrs, phas, bgrs)\n\n        # Pause\n        if random.random() < self.prob_pause:\n            fgrs, phas, bgrs = self._motion_pause(fgrs, phas, bgrs)\n        \n        return fgrs, phas, bgrs\n    \n    def _static_affine(self, *imgs, scale_ranges):\n        params = transforms.RandomAffine.get_params(\n            degrees=(-10, 10), translate=(0.1, 0.1), scale_ranges=scale_ranges,\n            shears=(-5, 5), img_size=imgs[0][0].size)\n        imgs = [[  FT.affine(t, *params,   FT.InterpolationMode.BILINEAR) for t in img] for img in imgs]\n        return imgs if len(imgs) > 1 else imgs[0] \n    \n    def _motion_affine(self, *imgs):\n        config = dict(degrees=(-10, 10), translate=(0.1, 0.1),\n                      scale_ranges=(0.9, 1.1), shears=(-5, 5), img_size=imgs[0][0].size)\n        angleA, (transXA, transYA), scaleA, (shearXA, shearYA) = transforms.RandomAffine.get_params(**config)\n        angleB, (transXB, transYB), scaleB, (shearXB, shearYB) = transforms.RandomAffine.get_params(**config)\n        \n        T = len(imgs[0])\n        easing = random_easing_fn()\n        for t in range(T):\n            percentage = easing(t / (T - 1))\n            angle = lerp(angleA, angleB, percentage)\n            transX = lerp(transXA, transXB, percentage)\n            transY = lerp(transYA, transYB, percentage)\n            scale = lerp(scaleA, scaleB, percentage)\n            shearX = lerp(shearXA, shearXB, percentage)\n            shearY = lerp(shearYA, shearYB, percentage)\n            for img in imgs:\n                img[t] =   FT.affine(img[t], angle, (transX, transY), scale, (shearX, shearY),   FT.InterpolationMode.BILINEAR)\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_noise(self, *imgs):\n        grain_size = random.random() * 3 + 1 # range 1 ~ 4\n        monochrome = random.random() < 0.5\n        for img in imgs:\n            T, C, H, W = img.shape\n            noise = torch.randn((T, 1 if monochrome else C, round(H / grain_size), round(W / grain_size)))\n            noise.mul_(random.random() * 0.2 / grain_size)\n            if grain_size != 1:\n                noise =   FT.resize(noise, (H, W))\n            img.add_(noise).clamp_(0, 1)\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_color_jitter(self, *imgs):\n        brightnessA, brightnessB, contrastA, contrastB, saturationA, saturationB, hueA, hueB \\\n            = torch.randn(8).mul(0.1).tolist()\n        strength = random.random() * 0.2\n        easing = random_easing_fn()\n        T = len(imgs[0])\n        for t in range(T):\n            percentage = easing(t / (T - 1)) * strength\n            for img in imgs:\n                img[t] =   FT.adjust_brightness(img[t], max(1 + lerp(brightnessA, brightnessB, percentage), 0.1))\n                img[t] =   FT.adjust_contrast(img[t], max(1 + lerp(contrastA, contrastB, percentage), 0.1))\n                img[t] =   FT.adjust_saturation(img[t], max(1 + lerp(brightnessA, brightnessB, percentage), 0.1))\n                img[t] =   FT.adjust_hue(img[t], min(0.5, max(-0.5, lerp(hueA, hueB, percentage) * 0.1)))\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_blur(self, *imgs):\n        blurA = random.random() * 10\n        blurB = random.random() * 10\n\n        T = len(imgs[0])\n        easing = random_easing_fn()\n        for t in range(T):\n            percentage = easing(t / (T - 1))\n            blur = max(lerp(blurA, blurB, percentage), 0)\n            if blur != 0:\n                kernel_size = int(blur * 2)\n                if kernel_size % 2 == 0:\n                    kernel_size += 1 # Make kernel_size odd\n                for img in imgs:\n                    img[t] =   FT.gaussian_blur(img[t], kernel_size, sigma=blur)\n    \n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_pause(self, *imgs):\n        T = len(imgs[0])\n        pause_frame = random.choice(range(T - 1))\n        pause_length = random.choice(range(T - pause_frame))\n        for img in imgs:\n            img[pause_frame + 1 : pause_frame + pause_length] = img[pause_frame]\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n\ndef lerp(a, b, percentage):\n    return a * (1 - percentage) + b * percentage\n\n\n\nclass Step: # Custom easing function for sudden change.\n    def __call__(self, value):\n        return 0 if value < 0.5 else 1\n\n\n# ---------------------------- Frame Sampler ----------------------------\n\n\nclass TrainFrameSampler:\n    def __init__(self, speed=[0.5, 1, 2, 3, 4, 5]):\n        self.speed = speed\n    \n    def __call__(self, seq_length):\n        frames = list(range(seq_length))\n        \n        # Speed up\n        speed = random.choice(self.speed)\n        frames = [int(f * speed) for f in frames]\n        \n        # Shift\n        shift = random.choice(range(seq_length))\n        frames = [f + shift for f in frames]\n        \n        # Reverse\n        if random.random() < 0.5:\n            frames = frames[::-1]\n\n        return frames\n    \nclass ValidFrameSampler:\n    def __call__(self, seq_length):\n        return range(seq_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.283951Z","iopub.execute_input":"2025-06-14T18:11:13.284215Z","iopub.status.idle":"2025-06-14T18:11:13.311971Z","shell.execute_reply.started":"2025-06-14T18:11:13.284199Z","shell.execute_reply":"2025-06-14T18:11:13.311389Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class VideoMatteDataset(Dataset):\n    def __init__(self,\n                 videomatte_dir,\n                 background_image_dir,\n                 size,\n                 seq_length,\n                 seq_sampler,\n                 transform=None):\n        self.background_image_dir = background_image_dir\n        self.background_image_files = os.listdir(background_image_dir)\n        \n        self.videomatte_dir = videomatte_dir\n        self.videomatte_clips = sorted(os.listdir(os.path.join(videomatte_dir, 'fgr')))\n        self.videomatte_frames = [sorted(os.listdir(os.path.join(videomatte_dir, 'fgr', clip))) \n                                  for clip in self.videomatte_clips]\n        self.videomatte_idx = [(clip_idx, frame_idx) \n                               for clip_idx in range(len(self.videomatte_clips)) \n                               for frame_idx in range(0, len(self.videomatte_frames[clip_idx]), seq_length)]\n        self.size = size\n        self.seq_length = seq_length\n        self.seq_sampler = seq_sampler\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.videomatte_idx)\n    \n    def __getitem__(self, idx):\n        bgrs = self._get_random_image_background()\n\n        \n        fgrs, phas = self._get_videomatte(idx)\n        \n        if self.transform is not None:\n            return self.transform(fgrs, phas, bgrs)\n        \n        return fgrs, phas, bgrs\n    \n    def _get_random_image_background(self):\n        with Image.open(os.path.join(self.background_image_dir, random.choice(self.background_image_files))) as bgr:\n            bgr = self._downsample_if_needed(bgr.convert('RGB'))\n        bgrs = [bgr] * self.seq_length\n        return bgrs\n    \n    \n    def _get_videomatte(self, idx):\n        clip_idx, frame_idx = self.videomatte_idx[idx]\n        clip = self.videomatte_clips[clip_idx]\n        frame_count = len(self.videomatte_frames[clip_idx])\n        fgrs, phas = [], []\n        for i in self.seq_sampler(self.seq_length):\n            frame = self.videomatte_frames[clip_idx][(frame_idx + i) % frame_count]\n            with Image.open(os.path.join(self.videomatte_dir, 'fgr', clip, frame)) as fgr, \\\n                 Image.open(os.path.join(self.videomatte_dir, 'pha', clip, frame)) as pha:\n                    fgr = self._downsample_if_needed(fgr.convert('RGB'))\n                    pha = self._downsample_if_needed(pha.convert('L'))\n            fgrs.append(fgr)\n            phas.append(pha)\n        return fgrs, phas\n    \n    def _downsample_if_needed(self, img):\n        w, h = img.size\n        if min(w, h) > self.size:\n            scale = self.size / min(w, h)\n            w = int(scale * w)\n            h = int(scale * h)\n            img = img.resize((w, h))\n        return img\n\nclass VideoMatteTrainAugmentation(MotionAugmentation):\n    def __init__(self, size):\n        super().__init__(\n            size=size,\n            prob_fgr_affine=0.3,\n            prob_bgr_affine=0.3,\n            prob_noise=0.1,\n            prob_color_jitter=0.3,\n            prob_grayscale=0.02,\n            prob_sharpness=0.1,\n            prob_blur=0.02,\n            prob_hflip=0.5,\n            prob_pause=0.03,\n        )\n\nclass VideoMatteValidAugmentation(MotionAugmentation):\n    def __init__(self, size):\n        super().__init__(\n            size=size,\n            prob_fgr_affine=0,\n            prob_bgr_affine=0,\n            prob_noise=0,\n            prob_color_jitter=0,\n            prob_grayscale=0,\n            prob_sharpness=0,\n            prob_blur=0,\n            prob_hflip=0,\n            prob_pause=0,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.312610Z","iopub.execute_input":"2025-06-14T18:11:13.312820Z","iopub.status.idle":"2025-06-14T18:11:13.330414Z","shell.execute_reply.started":"2025-06-14T18:11:13.312806Z","shell.execute_reply":"2025-06-14T18:11:13.329761Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class MotionAugmentation:\n    def __init__(self,\n                 size,\n                 prob_fgr_affine,\n                 prob_bgr_affine,\n                 prob_noise,\n                 prob_color_jitter,\n                 prob_grayscale,\n                 prob_sharpness,\n                 prob_blur,\n                 prob_hflip,\n                 prob_pause,\n                 static_affine=True,\n                 aspect_ratio_range=(0.9, 1.1)):\n        self.size = size\n        self.prob_fgr_affine = prob_fgr_affine\n        self.prob_bgr_affine = prob_bgr_affine\n        self.prob_noise = prob_noise\n        self.prob_color_jitter = prob_color_jitter\n        self.prob_grayscale = prob_grayscale\n        self.prob_sharpness = prob_sharpness\n        self.prob_blur = prob_blur\n        self.prob_hflip = prob_hflip\n        self.prob_pause = prob_pause\n        self.static_affine = static_affine\n        self.aspect_ratio_range = aspect_ratio_range\n        \n    def __call__(self, fgrs, phas, bgrs):\n        # Foreground affine\n        if random.random() < self.prob_fgr_affine:\n            fgrs, phas = self._motion_affine(fgrs, phas)\n\n        # Background affine\n        if random.random() < self.prob_bgr_affine / 2:\n            bgrs = self._motion_affine(bgrs)\n        if random.random() < self.prob_bgr_affine / 2:\n            fgrs, phas, bgrs = self._motion_affine(fgrs, phas, bgrs)\n                \n        # Still Affine\n        if self.static_affine:\n            fgrs, phas = self._static_affine(fgrs, phas, scale_ranges=(0.5, 1))\n            bgrs = self._static_affine(bgrs, scale_ranges=(1, 1.5))\n        \n        # To tensor\n        fgrs = torch.stack([FT.to_tensor(fgr) for fgr in fgrs])\n        phas = torch.stack([FT.to_tensor(pha) for pha in phas])\n        bgrs = torch.stack([FT.to_tensor(bgr) for bgr in bgrs])\n        \n        # Resize\n        params = transforms.RandomResizedCrop.get_params(fgrs, scale=(1, 1), ratio=self.aspect_ratio_range)\n        fgrs = FT.resized_crop(fgrs, *params, self.size, interpolation=FT.InterpolationMode.BILINEAR)\n        phas = FT.resized_crop(phas, *params, self.size, interpolation=FT.InterpolationMode.BILINEAR)\n        params = transforms.RandomResizedCrop.get_params(bgrs, scale=(1, 1), ratio=self.aspect_ratio_range)\n        bgrs = FT.resized_crop(bgrs, *params, self.size, interpolation=FT.InterpolationMode.BILINEAR)\n\n        # Horizontal flip\n        if random.random() < self.prob_hflip:\n            fgrs = FT.hflip(fgrs)\n            phas = FT.hflip(phas)\n        if random.random() < self.prob_hflip:\n            bgrs = FT.hflip(bgrs)\n\n        # Noise\n        if random.random() < self.prob_noise:\n            fgrs, bgrs = self._motion_noise(fgrs, bgrs)\n        \n        # Color jitter\n        if random.random() < self.prob_color_jitter:\n            fgrs = self._motion_color_jitter(fgrs)\n        if random.random() < self.prob_color_jitter:\n            bgrs = self._motion_color_jitter(bgrs)\n            \n        # Grayscale\n        if random.random() < self.prob_grayscale:\n            fgrs = FT.rgb_to_grayscale(fgrs, num_output_channels=3).contiguous()\n            bgrs = FT.rgb_to_grayscale(bgrs, num_output_channels=3).contiguous()\n            \n        # Sharpen\n        if random.random() < self.prob_sharpness:\n            sharpness = random.random() * 8\n            fgrs = FT.adjust_sharpness(fgrs, sharpness)\n            phas = FT.adjust_sharpness(phas, sharpness)\n            bgrs = FT.adjust_sharpness(bgrs, sharpness)\n        \n        # Blur\n        if random.random() < self.prob_blur / 3:\n            fgrs, phas = self._motion_blur(fgrs, phas)\n        if random.random() < self.prob_blur / 3:\n            bgrs = self._motion_blur(bgrs)\n        if random.random() < self.prob_blur / 3:\n            fgrs, phas, bgrs = self._motion_blur(fgrs, phas, bgrs)\n\n        # Pause\n        if random.random() < self.prob_pause:\n            fgrs, phas, bgrs = self._motion_pause(fgrs, phas, bgrs)\n        \n        return fgrs, phas, bgrs\n    \n    def _static_affine(self, *imgs, scale_ranges):\n        params = transforms.RandomAffine.get_params(\n            degrees=(-10, 10), translate=(0.1, 0.1), scale_ranges=scale_ranges,\n            shears=(-5, 5), img_size=imgs[0][0].size)\n        imgs = [[FT.affine(t, *params, FT.InterpolationMode.BILINEAR) for t in img] for img in imgs]\n        return imgs if len(imgs) > 1 else imgs[0] \n    \n    def _motion_affine(self, *imgs):\n        config = dict(degrees=(-10, 10), translate=(0.1, 0.1),\n                      scale_ranges=(0.9, 1.1), shears=(-5, 5), img_size=imgs[0][0].size)\n        angleA, (transXA, transYA), scaleA, (shearXA, shearYA) = transforms.RandomAffine.get_params(**config)\n        angleB, (transXB, transYB), scaleB, (shearXB, shearYB) = transforms.RandomAffine.get_params(**config)\n        \n        T = len(imgs[0])\n        easing = random_easing_fn()\n        for t in range(T):\n            percentage = easing(t / (T - 1))\n            angle = lerp(angleA, angleB, percentage)\n            transX = lerp(transXA, transXB, percentage)\n            transY = lerp(transYA, transYB, percentage)\n            scale = lerp(scaleA, scaleB, percentage)\n            shearX = lerp(shearXA, shearXB, percentage)\n            shearY = lerp(shearYA, shearYB, percentage)\n            for img in imgs:\n                img[t] = FT.affine(img[t], angle, (transX, transY), scale, (shearX, shearY), FT.InterpolationMode.BILINEAR)\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_noise(self, *imgs):\n        grain_size = random.random() * 3 + 1 # range 1 ~ 4\n        monochrome = random.random() < 0.5\n        for img in imgs:\n            T, C, H, W = img.shape\n            noise = torch.randn((T, 1 if monochrome else C, round(H / grain_size), round(W / grain_size)))\n            noise.mul_(random.random() * 0.2 / grain_size)\n            if grain_size != 1:\n                noise = FT.resize(noise, (H, W))\n            img.add_(noise).clamp_(0, 1)\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_color_jitter(self, *imgs):\n        brightnessA, brightnessB, contrastA, contrastB, saturationA, saturationB, hueA, hueB \\\n            = torch.randn(8).mul(0.1).tolist()\n        strength = random.random() * 0.2\n        easing = random_easing_fn()\n        T = len(imgs[0])\n        for t in range(T):\n            percentage = easing(t / (T - 1)) * strength\n            for img in imgs:\n                img[t] = FT.adjust_brightness(img[t], max(1 + lerp(brightnessA, brightnessB, percentage), 0.1))\n                img[t] = FT.adjust_contrast(img[t], max(1 + lerp(contrastA, contrastB, percentage), 0.1))\n                img[t] = FT.adjust_saturation(img[t], max(1 + lerp(brightnessA, brightnessB, percentage), 0.1))\n                img[t] = FT.adjust_hue(img[t], min(0.5, max(-0.5, lerp(hueA, hueB, percentage) * 0.1)))\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_blur(self, *imgs):\n        blurA = random.random() * 10\n        blurB = random.random() * 10\n\n        T = len(imgs[0])\n        easing = random_easing_fn()\n        for t in range(T):\n            percentage = easing(t / (T - 1))\n            blur = max(lerp(blurA, blurB, percentage), 0)\n            if blur != 0:\n                kernel_size = int(blur * 2)\n                if kernel_size % 2 == 0:\n                    kernel_size += 1 # Make kernel_size odd\n                for img in imgs:\n                    img[t] = FT.gaussian_blur(img[t], kernel_size, sigma=blur)\n    \n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_pause(self, *imgs):\n        T = len(imgs[0])\n        pause_frame = random.choice(range(T - 1))\n        pause_length = random.choice(range(T - pause_frame))\n        for img in imgs:\n            img[pause_frame + 1 : pause_frame + pause_length] = img[pause_frame]\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n\ndef lerp(a, b, percentage):\n    return a * (1 - percentage) + b * percentage\n\n\ndef random_easing_fn():\n    if random.random() < 0.2:\n        return ef.LinearInOut()\n    else:\n        return random.choice([\n            ef.BackEaseIn,\n            ef.BackEaseOut,\n            ef.BackEaseInOut,\n            ef.BounceEaseIn,\n            ef.BounceEaseOut,\n            ef.BounceEaseInOut,\n            ef.CircularEaseIn,\n            ef.CircularEaseOut,\n            ef.CircularEaseInOut,\n            ef.CubicEaseIn,\n            ef.CubicEaseOut,\n            ef.CubicEaseInOut,\n            ef.ExponentialEaseIn,\n            ef.ExponentialEaseOut,\n            ef.ExponentialEaseInOut,\n            ef.ElasticEaseIn,\n            ef.ElasticEaseOut,\n            ef.ElasticEaseInOut,\n            ef.QuadEaseIn,\n            ef.QuadEaseOut,\n            ef.QuadEaseInOut,\n            ef.QuarticEaseIn,\n            ef.QuarticEaseOut,\n            ef.QuarticEaseInOut,\n            ef.QuinticEaseIn,\n            ef.QuinticEaseOut,\n            ef.QuinticEaseInOut,\n            ef.SineEaseIn,\n            ef.SineEaseOut,\n            ef.SineEaseInOut,\n            Step,\n        ])()\n\nclass Step: # Custom easing function for sudden change.\n    def __call__(self, value):\n        return 0 if value < 0.5 else 1\n\n\n# ---------------------------- Frame Sampler ----------------------------\n\n\nclass TrainFrameSampler:\n    def __init__(self, speed=[0.5, 1, 2, 3, 4, 5]):\n        self.speed = speed\n    \n    def __call__(self, seq_length):\n        frames = list(range(seq_length))\n        \n        # Speed up\n        speed = random.choice(self.speed)\n        frames = [int(FT * speed) for FT in frames]\n        \n        # Shift\n        shift = random.choice(range(seq_length))\n        frames = [FT + shift for FT in frames]\n        \n        # Reverse\n        if random.random() < 0.5:\n            frames = frames[::-1]\n\n        return frames\n    \nclass ValidFrameSampler:\n    def __call__(self, seq_length):\n        return range(seq_length)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.332492Z","iopub.execute_input":"2025-06-14T18:11:13.332886Z","iopub.status.idle":"2025-06-14T18:11:13.359859Z","shell.execute_reply.started":"2025-06-14T18:11:13.332868Z","shell.execute_reply":"2025-06-14T18:11:13.359198Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"\"\"\"\nAdopted from <https://github.com/wuhuikai/DeepGuidedFilter/>\n\"\"\"\n\nclass FastGuidedFilterRefiner(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.guilded_filter = FastGuidedFilter(1)\n    \n    def forward_single_frame(self, fine_src, base_src, base_fgr, base_pha):\n        fine_src_gray = fine_src.mean(1, keepdim=True)\n        base_src_gray = base_src.mean(1, keepdim=True)\n        \n        fgr, pha = self.guilded_filter(\n            torch.cat([base_src, base_src_gray], dim=1),\n            torch.cat([base_fgr, base_pha], dim=1),\n            torch.cat([fine_src, fine_src_gray], dim=1)).split([3, 1], dim=1)\n        \n        return fgr, pha\n    \n    def forward_time_series(self, fine_src, base_src, base_fgr, base_pha):\n        B, T = fine_src.shape[:2]\n        fgr, pha = self.forward_single_frame(\n            fine_src.flatten(0, 1),\n            base_src.flatten(0, 1),\n            base_fgr.flatten(0, 1),\n            base_pha.flatten(0, 1))\n        fgr = fgr.unflatten(0, (B, T))\n        pha = pha.unflatten(0, (B, T))\n        return fgr, pha\n    \n    def forward(self, fine_src, base_src, base_fgr, base_pha, base_hid):\n        if fine_src.ndim == 5:\n            return self.forward_time_series(fine_src, base_src, base_fgr, base_pha)\n        else:\n            return self.forward_single_frame(fine_src, base_src, base_fgr, base_pha)\n\n\nclass FastGuidedFilter(nn.Module):\n    def __init__(self, r: int, eps: float = 1e-5):\n        super().__init__()\n        self.r = r\n        self.eps = eps\n        self.boxfilter = BoxFilter(r)\n\n    def forward(self, lr_x, lr_y, hr_x):\n        mean_x = self.boxfilter(lr_x)\n        mean_y = self.boxfilter(lr_y)\n        cov_xy = self.boxfilter(lr_x * lr_y) - mean_x * mean_y\n        var_x = self.boxfilter(lr_x * lr_x) - mean_x * mean_x\n        A = cov_xy / (var_x + self.eps)\n        b = mean_y - A * mean_x\n        A = F.interpolate(A, hr_x.shape[2:], mode='bilinear', align_corners=False)\n        b = F.interpolate(b, hr_x.shape[2:], mode='bilinear', align_corners=False)\n        return A * hr_x + b\n\n\nclass BoxFilter(nn.Module):\n    def __init__(self, r):\n        super(BoxFilter, self).__init__()\n        self.r = r\n\n    def forward(self, x):\n        # Note: The original implementation at <https://github.com/wuhuikai/DeepGuidedFilter/>\n        #       uses faster box blur. However, it may not be friendly for ONNX export.\n        #       We are switching to use simple convolution for box blur.\n        kernel_size = 2 * self.r + 1\n        kernel_x = torch.full((x.data.shape[1], 1, 1, kernel_size), 1 / kernel_size, device=x.device, dtype=x.dtype)\n        kernel_y = torch.full((x.data.shape[1], 1, kernel_size, 1), 1 / kernel_size, device=x.device, dtype=x.dtype)\n        x = F.conv2d(x, kernel_x, padding=(0, self.r), groups=x.data.shape[1])\n        x = F.conv2d(x, kernel_y, padding=(self.r, 0), groups=x.data.shape[1])\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.360482Z","iopub.execute_input":"2025-06-14T18:11:13.360673Z","iopub.status.idle":"2025-06-14T18:11:13.381845Z","shell.execute_reply.started":"2025-06-14T18:11:13.360635Z","shell.execute_reply":"2025-06-14T18:11:13.381112Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"class RecurrentDecoder(nn.Module):\n    def __init__(self, feature_channels, decoder_channels):\n        super().__init__()\n        self.avgpool = AvgPool()\n        self.decode4 = BottleneckBlock(feature_channels[3])\n        self.decode3 = UpsamplingBlock(feature_channels[3], feature_channels[2], 3, decoder_channels[0])\n        self.decode2 = UpsamplingBlock(decoder_channels[0], feature_channels[1], 3, decoder_channels[1])\n        self.decode1 = UpsamplingBlock(decoder_channels[1], feature_channels[0], 3, decoder_channels[2])\n        self.decode0 = OutputBlock(decoder_channels[2], 3, decoder_channels[3])\n\n    def forward(self,\n                s0: Tensor, f1: Tensor, f2: Tensor, f3: Tensor, f4: Tensor,\n                r1: Optional[Tensor], r2: Optional[Tensor],\n                r3: Optional[Tensor], r4: Optional[Tensor]):\n        s1, s2, s3 = self.avgpool(s0)\n        x4, r4 = self.decode4(f4, r4)\n        x3, r3 = self.decode3(x4, f3, s3, r3)\n        x2, r2 = self.decode2(x3, f2, s2, r2)\n        x1, r1 = self.decode1(x2, f1, s1, r1)\n        x0 = self.decode0(x1, s0)\n        return x0, r1, r2, r3, r4\n    \n\nclass AvgPool(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.avgpool = nn.AvgPool2d(2, 2, count_include_pad=False, ceil_mode=True)\n        \n    def forward_single_frame(self, s0):\n        s1 = self.avgpool(s0)\n        s2 = self.avgpool(s1)\n        s3 = self.avgpool(s2)\n        return s1, s2, s3\n    \n    def forward_time_series(self, s0):\n        B, T = s0.shape[:2]\n        s0 = s0.flatten(0, 1)\n        s1, s2, s3 = self.forward_single_frame(s0)\n        s1 = s1.unflatten(0, (B, T))\n        s2 = s2.unflatten(0, (B, T))\n        s3 = s3.unflatten(0, (B, T))\n        return s1, s2, s3\n    \n    def forward(self, s0):\n        if s0.ndim == 5:\n            return self.forward_time_series(s0)\n        else:\n            return self.forward_single_frame(s0)\n\n\nclass BottleneckBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.channels = channels\n        self.gru = ConvGRU(channels // 2)\n        \n    def forward(self, x, r: Optional[Tensor]):\n        a, b = x.split(self.channels // 2, dim=-3)\n        b, r = self.gru(b, r)\n        x = torch.cat([a, b], dim=-3)\n        return x, r\n\n    \nclass UpsamplingBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, src_channels, out_channels):\n        super().__init__()\n        self.out_channels = out_channels\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels + skip_channels + src_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n        )\n        self.gru = ConvGRU(out_channels // 2)\n\n    def forward_single_frame(self, x, f, s, r: Optional[Tensor]):\n        x = self.upsample(x)\n        x = x[:, :, :s.size(2), :s.size(3)]\n        x = torch.cat([x, f, s], dim=1)\n        x = self.conv(x)\n        a, b = x.split(self.out_channels // 2, dim=1)\n        b, r = self.gru(b, r)\n        x = torch.cat([a, b], dim=1)\n        return x, r\n    \n    def forward_time_series(self, x, f, s, r: Optional[Tensor]):\n        B, T, _, H, W = s.shape\n        x = x.flatten(0, 1)\n        f = f.flatten(0, 1)\n        s = s.flatten(0, 1)\n        x = self.upsample(x)\n        x = x[:, :, :H, :W]\n        x = torch.cat([x, f, s], dim=1)\n        x = self.conv(x)\n        x = x.unflatten(0, (B, T))\n        a, b = x.split(self.out_channels // 2, dim=2)\n        b, r = self.gru(b, r)\n        x = torch.cat([a, b], dim=2)\n        return x, r\n    \n    def forward(self, x, f, s, r: Optional[Tensor]):\n        if x.ndim == 5:\n            return self.forward_time_series(x, f, s, r)\n        else:\n            return self.forward_single_frame(x, f, s, r)\n\n\nclass OutputBlock(nn.Module):\n    def __init__(self, in_channels, src_channels, out_channels):\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels + src_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n        )\n        \n    def forward_single_frame(self, x, s):\n        x = self.upsample(x)\n        x = x[:, :, :s.size(2), :s.size(3)]\n        x = torch.cat([x, s], dim=1)\n        x = self.conv(x)\n        return x\n    \n    def forward_time_series(self, x, s):\n        B, T, _, H, W = s.shape\n        x = x.flatten(0, 1)\n        s = s.flatten(0, 1)\n        x = self.upsample(x)\n        x = x[:, :, :H, :W]\n        x = torch.cat([x, s], dim=1)\n        x = self.conv(x)\n        x = x.unflatten(0, (B, T))\n        return x\n    \n    def forward(self, x, s):\n        if x.ndim == 5:\n            return self.forward_time_series(x, s)\n        else:\n            return self.forward_single_frame(x, s)\n\n\nclass ConvGRU(nn.Module):\n    def __init__(self,\n                 channels: int,\n                 kernel_size: int = 3,\n                 padding: int = 1):\n        super().__init__()\n        self.channels = channels\n        self.ih = nn.Sequential(\n            nn.Conv2d(channels * 2, channels * 2, kernel_size, padding=padding),\n            nn.Sigmoid()\n        )\n        self.hh = nn.Sequential(\n            nn.Conv2d(channels * 2, channels, kernel_size, padding=padding),\n            nn.Tanh()\n        )\n        \n    def forward_single_frame(self, x, h):\n        r, z = self.ih(torch.cat([x, h], dim=1)).split(self.channels, dim=1)\n        c = self.hh(torch.cat([x, r * h], dim=1))\n        h = (1 - z) * h + z * c\n        return h, h\n    \n    def forward_time_series(self, x, h):\n        o = []\n        for xt in x.unbind(dim=1):\n            ot, h = self.forward_single_frame(xt, h)\n            o.append(ot)\n        o = torch.stack(o, dim=1)\n        return o, h\n        \n    def forward(self, x, h: Optional[Tensor]):\n        if h is None:\n            h = torch.zeros((x.size(0), x.size(-3), x.size(-2), x.size(-1)),\n                            device=x.device, dtype=x.dtype)\n        \n        if x.ndim == 5:\n            return self.forward_time_series(x, h)\n        else:\n            return self.forward_single_frame(x, h)\n\n\nclass Projection(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, 1)\n    \n    def forward_single_frame(self, x):\n        return self.conv(x)\n    \n    def forward_time_series(self, x):\n        B, T = x.shape[:2]\n        return self.conv(x.flatten(0, 1)).unflatten(0, (B, T))\n        \n    def forward(self, x):\n        if x.ndim == 5:\n            return self.forward_time_series(x)\n        else:\n            return self.forward_single_frame(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.382823Z","iopub.execute_input":"2025-06-14T18:11:13.383059Z","iopub.status.idle":"2025-06-14T18:11:13.408925Z","shell.execute_reply.started":"2025-06-14T18:11:13.383035Z","shell.execute_reply":"2025-06-14T18:11:13.408359Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class MobileNetV3LargeEncoder(MobileNetV3):\n    def __init__(self, pretrained: bool = False):\n        super().__init__(\n            inverted_residual_setting=[\n                InvertedResidualConfig( 16, 3,  16,  16, False, \"RE\", 1, 1, 1),\n                InvertedResidualConfig( 16, 3,  64,  24, False, \"RE\", 2, 1, 1),  # C1\n                InvertedResidualConfig( 24, 3,  72,  24, False, \"RE\", 1, 1, 1),\n                InvertedResidualConfig( 24, 5,  72,  40,  True, \"RE\", 2, 1, 1),  # C2\n                InvertedResidualConfig( 40, 5, 120,  40,  True, \"RE\", 1, 1, 1),\n                InvertedResidualConfig( 40, 5, 120,  40,  True, \"RE\", 1, 1, 1),\n                InvertedResidualConfig( 40, 3, 240,  80, False, \"HS\", 2, 1, 1),  # C3\n                InvertedResidualConfig( 80, 3, 200,  80, False, \"HS\", 1, 1, 1),\n                InvertedResidualConfig( 80, 3, 184,  80, False, \"HS\", 1, 1, 1),\n                InvertedResidualConfig( 80, 3, 184,  80, False, \"HS\", 1, 1, 1),\n                InvertedResidualConfig( 80, 3, 480, 112,  True, \"HS\", 1, 1, 1),\n                InvertedResidualConfig(112, 3, 672, 112,  True, \"HS\", 1, 1, 1),\n                InvertedResidualConfig(112, 5, 672, 160,  True, \"HS\", 2, 2, 1),  # C4\n                InvertedResidualConfig(160, 5, 960, 160,  True, \"HS\", 1, 2, 1),\n                InvertedResidualConfig(160, 5, 960, 160,  True, \"HS\", 1, 2, 1),\n            ],\n            last_channel=1280\n        )\n        \n        if pretrained:\n            self.load_state_dict(torch.hub.load_state_dict_from_url(\n                'https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth'))\n\n        del self.avgpool\n        del self.classifier\n        \n    def forward_single_frame(self, x):\n        x = normalize(x, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        \n        x = self.features[0](x)\n        x = self.features[1](x)\n        f1 = x\n        x = self.features[2](x)\n        x = self.features[3](x)\n        f2 = x\n        x = self.features[4](x)\n        x = self.features[5](x)\n        x = self.features[6](x)\n        f3 = x\n        x = self.features[7](x)\n        x = self.features[8](x)\n        x = self.features[9](x)\n        x = self.features[10](x)\n        x = self.features[11](x)\n        x = self.features[12](x)\n        x = self.features[13](x)\n        x = self.features[14](x)\n        x = self.features[15](x)\n        x = self.features[16](x)\n        f4 = x\n        return [f1, f2, f3, f4]\n    \n    def forward_time_series(self, x):\n        B, T = x.shape[:2]\n        features = self.forward_single_frame(x.flatten(0, 1))\n        features = [f.unflatten(0, (B, T)) for f in features]\n        return features\n\n    def forward(self, x):\n        if x.ndim == 5:\n            return self.forward_time_series(x)\n        else:\n            return self.forward_single_frame(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.409693Z","iopub.execute_input":"2025-06-14T18:11:13.409986Z","iopub.status.idle":"2025-06-14T18:11:13.428801Z","shell.execute_reply.started":"2025-06-14T18:11:13.409959Z","shell.execute_reply":"2025-06-14T18:11:13.428201Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class LRASPP(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.aspp1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n        self.aspp2 = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.Sigmoid()\n        )\n        \n    def forward_single_frame(self, x):\n        return self.aspp1(x) * self.aspp2(x)\n    \n    def forward_time_series(self, x):\n        B, T = x.shape[:2]\n        x = self.forward_single_frame(x.flatten(0, 1)).unflatten(0, (B, T))\n        return x\n    \n    def forward(self, x):\n        if x.ndim == 5:\n            return self.forward_time_series(x)\n        else:\n            return self.forward_single_frame(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.430007Z","iopub.execute_input":"2025-06-14T18:11:13.430355Z","iopub.status.idle":"2025-06-14T18:11:13.449549Z","shell.execute_reply.started":"2025-06-14T18:11:13.430328Z","shell.execute_reply":"2025-06-14T18:11:13.448826Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class MattingNetwork(nn.Module):\n    def __init__(self,\n                 variant: str = 'mobilenetv3',\n                 refiner: str = 'deep_guided_filter',\n                 pretrained_backbone: bool = False):\n        super().__init__()\n        assert variant in ['mobilenetv3', 'resnet50']\n        assert refiner in ['fast_guided_filter', 'deep_guided_filter']\n        \n\n        self.backbone = MobileNetV3LargeEncoder(pretrained_backbone)\n        self.aspp = LRASPP(960, 128)\n        self.decoder = RecurrentDecoder([16, 24, 40, 128], [80, 40, 32, 16])\n\n            \n        self.project_mat = Projection(16, 4)\n        self.project_seg = Projection(16, 1)\n\n\n        self.refiner = FastGuidedFilterRefiner()\n        \n    def forward(self,\n                src: Tensor,\n                r1: Optional[Tensor] = None,\n                r2: Optional[Tensor] = None,\n                r3: Optional[Tensor] = None,\n                r4: Optional[Tensor] = None,\n                downsample_ratio: float = 1,\n                segmentation_pass: bool = False):\n        \n        if downsample_ratio != 1:\n            src_sm = self._interpolate(src, scale_factor=downsample_ratio)\n        else:\n            src_sm = src\n        \n        f1, f2, f3, f4 = self.backbone(src_sm)\n        f4 = self.aspp(f4)\n        hid, *rec = self.decoder(src_sm, f1, f2, f3, f4, r1, r2, r3, r4)\n        \n        if not segmentation_pass:\n            fgr_residual, pha = self.project_mat(hid).split([3, 1], dim=-3)\n            if downsample_ratio != 1:\n                fgr_residual, pha = self.refiner(src, src_sm, fgr_residual, pha, hid)\n            fgr = fgr_residual + src\n            fgr = fgr.clamp(0., 1.)\n            pha = pha.clamp(0., 1.)\n            return [fgr, pha, *rec]\n        else:\n            seg = self.project_seg(hid)\n            return [seg, *rec]\n\n    def _interpolate(self, x: Tensor, scale_factor: float):\n        if x.ndim == 5:\n            B, T = x.shape[:2]\n            x = F.interpolate(x.flatten(0, 1), scale_factor=scale_factor,\n                mode='bilinear', align_corners=False, recompute_scale_factor=False)\n            x = x.unflatten(0, (B, T))\n        else:\n            x = F.interpolate(x, scale_factor=scale_factor,\n                mode='bilinear', align_corners=False, recompute_scale_factor=False)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.450272Z","iopub.execute_input":"2025-06-14T18:11:13.450437Z","iopub.status.idle":"2025-06-14T18:11:13.465394Z","shell.execute_reply.started":"2025-06-14T18:11:13.450424Z","shell.execute_reply":"2025-06-14T18:11:13.464810Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, rank, world_size):\n        self.parse_args()\n        self.init_distributed(rank, world_size)\n        self.init_datasets()\n        self.init_model()\n        self.init_writer()\n        self.train()\n        self.cleanup()\n        \n    def parse_args(self):\n        parser = argparse.ArgumentParser()\n        # Model\n        parser.add_argument('--model-variant', type=str, required=True, choices=['mobilenetv3'])\n        # Matting dataset\n        parser.add_argument('--dataset', type=str, required=True, choices=['videomatte', 'imagematte'])\n        # Learning rate\n        parser.add_argument('--learning-rate-backbone', type=float, required=True)\n        parser.add_argument('--learning-rate-aspp', type=float, required=True)\n        parser.add_argument('--learning-rate-decoder', type=float, required=True)\n        parser.add_argument('--learning-rate-refiner', type=float, required=True)\n        # Training setting\n        parser.add_argument('--train-hr', action='store_true')\n        parser.add_argument('--resolution-lr', type=int, default=512)\n        parser.add_argument('--resolution-hr', type=int, default=2048)\n        parser.add_argument('--seq-length-lr', type=int, required=True)\n        parser.add_argument('--seq-length-hr', type=int, default=6)\n        parser.add_argument('--downsample-ratio', type=float, default=0.25)\n        parser.add_argument('--batch-size-per-gpu', type=int, default=1)\n        parser.add_argument('--num-workers', type=int, default=8)\n        parser.add_argument('--epoch-start', type=int, default=0)\n        parser.add_argument('--epoch-end', type=int, default=16)\n        # Tensorboard logging\n        parser.add_argument('--log-dir', type=str, required=True)\n        parser.add_argument('--log-train-loss-interval', type=int, default=20)\n        parser.add_argument('--log-train-images-interval', type=int, default=500)\n        # Checkpoint loading and saving\n        parser.add_argument('--checkpoint', type=str)\n        parser.add_argument('--checkpoint-dir', type=str, required=True)\n        parser.add_argument('--checkpoint-save-interval', type=int, default=500)\n        # Distributed\n        parser.add_argument('--distributed-addr', type=str, default='localhost')\n        parser.add_argument('--distributed-port', type=str, default='12355')\n        # Debugging\n        parser.add_argument('--disable-progress-bar', action='store_true')\n        parser.add_argument('--disable-validation', action='store_true')\n        parser.add_argument('--disable-mixed-precision', action='store_true')\n        self.args = parser.parse_args()\n        \n    def init_distributed(self, rank, world_size):\n        self.rank = rank\n        self.world_size = world_size\n        self.log('Initializing distributed')\n        os.environ['MASTER_ADDR'] = self.args.distributed_addr\n        os.environ['MASTER_PORT'] = self.args.distributed_port\n        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    \n    def init_datasets(self):\n        self.log('Initializing matting datasets')\n        size_hr = (self.args.resolution_hr, self.args.resolution_hr)\n        size_lr = (self.args.resolution_lr, self.args.resolution_lr)\n        \n        # Matting datasets:\n        if self.args.dataset == 'videomatte':\n            self.dataset_lr_train = VideoMatteDataset(\n                videomatte_dir=DATA_PATHS['videomatte']['train'],\n                background_image_dir=DATA_PATHS['background_images']['train'],\n                size=self.args.resolution_lr,\n                seq_length=self.args.seq_length_lr,\n                seq_sampler=TrainFrameSampler(),\n                transform=VideoMatteTrainAugmentation(size_lr))\n            if self.args.train_hr:\n                self.dataset_hr_train = VideoMatteDataset(\n                    videomatte_dir=DATA_PATHS['videomatte']['train'],\n                    background_image_dir=DATA_PATHS['background_images']['train'],\n                    size=self.args.resolution_hr,\n                    seq_length=self.args.seq_length_hr,\n                    seq_sampler=TrainFrameSampler(),\n                    transform=VideoMatteTrainAugmentation(size_hr))\n            self.dataset_valid = VideoMatteDataset(\n                videomatte_dir=DATA_PATHS['videomatte']['valid'],\n                background_image_dir=DATA_PATHS['background_images']['valid'],\n                size=self.args.resolution_hr if self.args.train_hr else self.args.resolution_lr,\n                seq_length=self.args.seq_length_hr if self.args.train_hr else self.args.seq_length_lr,\n                seq_sampler=ValidFrameSampler(),\n                transform=VideoMatteValidAugmentation(size_hr if self.args.train_hr else size_lr))\n        \n            \n        # Matting dataloaders:\n        self.datasampler_lr_train = DistributedSampler(\n            dataset=self.dataset_lr_train,\n            rank=self.rank,\n            num_replicas=self.world_size,\n            shuffle=True)\n        self.dataloader_lr_train = DataLoader(\n            dataset=self.dataset_lr_train,\n            batch_size=self.args.batch_size_per_gpu,\n            num_workers=self.args.num_workers,\n            sampler=self.datasampler_lr_train,\n            pin_memory=True)\n        if self.args.train_hr:\n            self.datasampler_hr_train = DistributedSampler(\n                dataset=self.dataset_hr_train,\n                rank=self.rank,\n                num_replicas=self.world_size,\n                shuffle=True)\n            self.dataloader_hr_train = DataLoader(\n                dataset=self.dataset_hr_train,\n                batch_size=self.args.batch_size_per_gpu,\n                num_workers=self.args.num_workers,\n                sampler=self.datasampler_hr_train,\n                pin_memory=True)\n        self.dataloader_valid = DataLoader(\n            dataset=self.dataset_valid,\n            batch_size=self.args.batch_size_per_gpu,\n            num_workers=self.args.num_workers,\n            pin_memory=True)\n        \n        # Segementation datasets\n        '''\n        self.log('Initializing image segmentation datasets')\n        self.dataset_seg_image = ConcatDataset([\n            CocoPanopticDataset(\n                imgdir=DATA_PATHS['coco_panoptic']['imgdir'],\n                anndir=DATA_PATHS['coco_panoptic']['anndir'],\n                annfile=DATA_PATHS['coco_panoptic']['annfile'],\n                transform=CocoPanopticTrainAugmentation(size_lr)),\n            SuperviselyPersonDataset(\n                imgdir=DATA_PATHS['spd']['imgdir'],\n                segdir=DATA_PATHS['spd']['segdir'],\n                transform=CocoPanopticTrainAugmentation(size_lr))\n        ])\n        \n        self.datasampler_seg_image = DistributedSampler(\n            dataset=self.dataset_seg_image,\n            rank=self.rank,\n            num_replicas=self.world_size,\n            shuffle=True)\n        self.dataloader_seg_image = DataLoader(\n            dataset=self.dataset_seg_image,\n            batch_size=self.args.batch_size_per_gpu * self.args.seq_length_lr,\n            num_workers=self.args.num_workers,\n            sampler=self.datasampler_seg_image,\n            pin_memory=True)\n        '''\n        \n        \n    def init_model(self):\n        self.log('Initializing model')\n        self.model = MattingNetwork(self.args.model_variant, pretrained_backbone=True).to(self.rank)\n        \n        if self.args.checkpoint:\n            self.log(f'Restoring from checkpoint: {self.args.checkpoint}')\n            self.log(self.model.load_state_dict(\n                torch.load(self.args.checkpoint, map_location=f'cuda:{self.rank}')))\n            \n        self.model = nn.SyncBatchNorm.convert_sync_batchnorm(self.model)\n        self.model_ddp = DDP(self.model, device_ids=[self.rank], broadcast_buffers=False, find_unused_parameters=True)\n        self.optimizer = Adam([\n            {'params': self.model.backbone.parameters(), 'lr': self.args.learning_rate_backbone},\n            {'params': self.model.aspp.parameters(), 'lr': self.args.learning_rate_aspp},\n            {'params': self.model.decoder.parameters(), 'lr': self.args.learning_rate_decoder},\n            {'params': self.model.project_mat.parameters(), 'lr': self.args.learning_rate_decoder},\n            {'params': self.model.project_seg.parameters(), 'lr': self.args.learning_rate_decoder},\n            {'params': self.model.refiner.parameters(), 'lr': self.args.learning_rate_refiner},\n        ])\n        self.scaler = GradScaler()\n        \n    def init_writer(self):\n        if self.rank == 0:\n            self.log('Initializing writer')\n            self.writer = SummaryWriter(self.args.log_dir)\n        \n    def train(self):\n        for epoch in range(self.args.epoch_start, self.args.epoch_end):\n            self.epoch = epoch\n            self.step = epoch * len(self.dataloader_lr_train)\n            \n            if not self.args.disable_validation:\n                self.validate()\n            \n            self.log(f'Training epoch: {epoch}')\n            for true_fgr, true_pha, true_bgr in tqdm(self.dataloader_lr_train, disable=self.args.disable_progress_bar, dynamic_ncols=True):\n                # Low resolution pass\n                self.train_mat(true_fgr, true_pha, true_bgr, downsample_ratio=1, tag='lr')\n\n                # High resolution pass\n                if self.args.train_hr:\n                    true_fgr, true_pha, true_bgr = self.load_next_mat_hr_sample()\n                    self.train_mat(true_fgr, true_pha, true_bgr, downsample_ratio=self.args.downsample_ratio, tag='hr')\n                '''\n                # Segmentation pass\n                if self.step % 2 == 0:\n                    true_img, true_seg = self.load_next_seg_video_sample()\n                    self.train_seg(true_img, true_seg, log_label='seg_video')\n                else:\n                    true_img, true_seg = self.load_next_seg_image_sample()\n                    self.train_seg(true_img.unsqueeze(1), true_seg.unsqueeze(1), log_label='seg_image')\n                '''\n                if self.step % self.args.checkpoint_save_interval == 0:\n                    self.save()\n                    \n                self.step += 1\n                \n    def train_mat(self, true_fgr, true_pha, true_bgr, downsample_ratio, tag):\n        true_fgr = true_fgr.to(self.rank, non_blocking=True)\n        true_pha = true_pha.to(self.rank, non_blocking=True)\n        true_bgr = true_bgr.to(self.rank, non_blocking=True)\n        true_fgr, true_pha, true_bgr = self.random_crop(true_fgr, true_pha, true_bgr)\n        true_src = true_fgr * true_pha + true_bgr * (1 - true_pha)\n        \n        with autocast(enabled=not self.args.disable_mixed_precision):\n            pred_fgr, pred_pha = self.model_ddp(true_src, downsample_ratio=downsample_ratio)[:2]\n            loss = matting_loss(pred_fgr, pred_pha, true_fgr, true_pha)\n\n        self.scaler.scale(loss['total']).backward()\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n        self.optimizer.zero_grad()\n        \n        if self.rank == 0 and self.step % self.args.log_train_loss_interval == 0:\n            for loss_name, loss_value in loss.items():\n                self.writer.add_scalar(f'train_{tag}_{loss_name}', loss_value, self.step)\n            \n        if self.rank == 0 and self.step % self.args.log_train_images_interval == 0:\n            self.writer.add_image(f'train_{tag}_pred_fgr', make_grid(pred_fgr.flatten(0, 1), nrow=pred_fgr.size(1)), self.step)\n            self.writer.add_image(f'train_{tag}_pred_pha', make_grid(pred_pha.flatten(0, 1), nrow=pred_pha.size(1)), self.step)\n            self.writer.add_image(f'train_{tag}_true_fgr', make_grid(true_fgr.flatten(0, 1), nrow=true_fgr.size(1)), self.step)\n            self.writer.add_image(f'train_{tag}_true_pha', make_grid(true_pha.flatten(0, 1), nrow=true_pha.size(1)), self.step)\n            self.writer.add_image(f'train_{tag}_true_src', make_grid(true_src.flatten(0, 1), nrow=true_src.size(1)), self.step)\n            \n    def train_seg(self, true_img, true_seg, log_label):\n        true_img = true_img.to(self.rank, non_blocking=True)\n        true_seg = true_seg.to(self.rank, non_blocking=True)\n        \n        true_img, true_seg = self.random_crop(true_img, true_seg)\n        \n        with autocast(enabled=not self.args.disable_mixed_precision):\n            pred_seg = self.model_ddp(true_img, segmentation_pass=True)[0]\n            loss = segmentation_loss(pred_seg, true_seg)\n        \n        self.scaler.scale(loss).backward()\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n        self.optimizer.zero_grad()\n        \n        if self.rank == 0 and (self.step - self.step % 2) % self.args.log_train_loss_interval == 0:\n            self.writer.add_scalar(f'{log_label}_loss', loss, self.step)\n        \n        if self.rank == 0 and (self.step - self.step % 2) % self.args.log_train_images_interval == 0:\n            self.writer.add_image(f'{log_label}_pred_seg', make_grid(pred_seg.flatten(0, 1).float().sigmoid(), nrow=self.args.seq_length_lr), self.step)\n            self.writer.add_image(f'{log_label}_true_seg', make_grid(true_seg.flatten(0, 1), nrow=self.args.seq_length_lr), self.step)\n            self.writer.add_image(f'{log_label}_true_img', make_grid(true_img.flatten(0, 1), nrow=self.args.seq_length_lr), self.step)\n    \n    def load_next_mat_hr_sample(self):\n        try:\n            sample = next(self.dataiterator_mat_hr)\n        except:\n            self.datasampler_hr_train.set_epoch(self.datasampler_hr_train.epoch + 1)\n            self.dataiterator_mat_hr = iter(self.dataloader_hr_train)\n            sample = next(self.dataiterator_mat_hr)\n        return sample\n    \n    def load_next_seg_video_sample(self):\n        try:\n            sample = next(self.dataiterator_seg_video)\n        except:\n            self.datasampler_seg_video.set_epoch(self.datasampler_seg_video.epoch + 1)\n            self.dataiterator_seg_video = iter(self.dataloader_seg_video)\n            sample = next(self.dataiterator_seg_video)\n        return sample\n    \n    def load_next_seg_image_sample(self):\n        try:\n            sample = next(self.dataiterator_seg_image)\n        except:\n            self.datasampler_seg_image.set_epoch(self.datasampler_seg_image.epoch + 1)\n            self.dataiterator_seg_image = iter(self.dataloader_seg_image)\n            sample = next(self.dataiterator_seg_image)\n        return sample\n    \n    def validate(self):\n        if self.rank == 0:\n            self.log(f'Validating at the start of epoch: {self.epoch}')\n            self.model_ddp.eval()\n            total_loss, total_count = 0, 0\n            with torch.no_grad():\n                with autocast(enabled=not self.args.disable_mixed_precision):\n                    for true_fgr, true_pha, true_bgr in tqdm(self.dataloader_valid, disable=self.args.disable_progress_bar, dynamic_ncols=True):\n                        true_fgr = true_fgr.to(self.rank, non_blocking=True)\n                        true_pha = true_pha.to(self.rank, non_blocking=True)\n                        true_bgr = true_bgr.to(self.rank, non_blocking=True)\n                        true_src = true_fgr * true_pha + true_bgr * (1 - true_pha)\n                        batch_size = true_src.size(0)\n                        pred_fgr, pred_pha = self.model(true_src)[:2]\n                        total_loss += matting_loss(pred_fgr, pred_pha, true_fgr, true_pha)['total'].item() * batch_size\n                        total_count += batch_size\n            avg_loss = total_loss / total_count\n            self.log(f'Validation set average loss: {avg_loss}')\n            self.writer.add_scalar('valid_loss', avg_loss, self.step)\n            self.model_ddp.train()\n        dist.barrier()\n    \n    def random_crop(self, *imgs):\n        h, w = imgs[0].shape[-2:]\n        w = random.choice(range(w // 2, w))\n        h = random.choice(range(h // 2, h))\n        results = []\n        for img in imgs:\n            B, T = img.shape[:2]\n            img = img.flatten(0, 1)\n            img = F.interpolate(img, (max(h, w), max(h, w)), mode='bilinear', align_corners=False)\n            img = center_crop(img, (h, w))\n            img = img.reshape(B, T, *img.shape[1:])\n            results.append(img)\n        return results\n    \n    def save(self):\n        if self.rank == 0:\n            os.makedirs(self.args.checkpoint_dir, exist_ok=True)\n            torch.save(self.model.state_dict(), os.path.join(self.args.checkpoint_dir, f'epoch-{self.epoch}.pth'))\n            self.log('Model saved')\n        dist.barrier()\n        \n    def cleanup(self):\n        dist.destroy_process_group()\n        \n    def log(self, msg):\n        print(f'[GPU{self.rank}] {msg}')\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.466318Z","iopub.execute_input":"2025-06-14T18:11:13.466545Z","iopub.status.idle":"2025-06-14T18:11:13.503512Z","shell.execute_reply.started":"2025-06-14T18:11:13.466530Z","shell.execute_reply":"2025-06-14T18:11:13.502702Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:13.504321Z","iopub.execute_input":"2025-06-14T18:11:13.504515Z","iopub.status.idle":"2025-06-14T18:11:13.523258Z","shell.execute_reply.started":"2025-06-14T18:11:13.504500Z","shell.execute_reply":"2025-06-14T18:11:13.522349Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_35/668683560.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"],"ename":"SyntaxError","evalue":"'break' outside loop (668683560.py, line 1)","output_type":"error"}],"execution_count":18},{"cell_type":"code","source":"sys.argv=[\n    '--model-variant', 'mobilenetv3',\n    '--dataset', 'videomatte',\n    '--resolution-lr', '512',\n    '--seq-length-lr', '15',\n    '--learning-rate-backbone', '0.0001',\n    '--learning-rate-aspp', '0.0002',\n    '--learning-rate-decoder', '0.0002',\n    '--learning-rate-refiner', '0',\n    '--checkpoint-dir', 'checkpoint/stage1',\n    '--log-dir', 'log/stage1',\n    '--epoch-start', '0',\n    '--epoch-end', '5']\n\nTrainer(0, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:11:53.802766Z","iopub.execute_input":"2025-06-14T18:11:53.803048Z","iopub.status.idle":"2025-06-14T18:11:53.812307Z","shell.execute_reply.started":"2025-06-14T18:11:53.803027Z","shell.execute_reply":"2025-06-14T18:11:53.811399Z"}},"outputs":[{"name":"stderr","text":"usage: --model-variant [-h] --model-variant {mobilenetv3} --dataset {videomatte,imagematte}\n                       --learning-rate-backbone LEARNING_RATE_BACKBONE --learning-rate-aspp\n                       LEARNING_RATE_ASPP --learning-rate-decoder LEARNING_RATE_DECODER\n                       --learning-rate-refiner LEARNING_RATE_REFINER [--train-hr]\n                       [--resolution-lr RESOLUTION_LR] [--resolution-hr RESOLUTION_HR]\n                       --seq-length-lr SEQ_LENGTH_LR [--seq-length-hr SEQ_LENGTH_HR]\n                       [--downsample-ratio DOWNSAMPLE_RATIO]\n                       [--batch-size-per-gpu BATCH_SIZE_PER_GPU] [--num-workers NUM_WORKERS]\n                       [--epoch-start EPOCH_START] [--epoch-end EPOCH_END] --log-dir LOG_DIR\n                       [--log-train-loss-interval LOG_TRAIN_LOSS_INTERVAL]\n                       [--log-train-images-interval LOG_TRAIN_IMAGES_INTERVAL]\n                       [--checkpoint CHECKPOINT] --checkpoint-dir CHECKPOINT_DIR\n                       [--checkpoint-save-interval CHECKPOINT_SAVE_INTERVAL]\n                       [--distributed-addr DISTRIBUTED_ADDR] [--distributed-port DISTRIBUTED_PORT]\n                       [--disable-progress-bar] [--disable-validation] [--disable-mixed-precision]\n--model-variant: error: the following arguments are required: --model-variant\n","output_type":"stream"},{"traceback":["An exception has occurred, use %tb to see the full traceback.\n","\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"],"ename":"SystemExit","evalue":"2","output_type":"error"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"code = \"\"\"\nimport gdown\nimport shutil\nimport tarfile\nimport argparse\nimport torch\nimport random\nimport os\nimport sys\nfrom torch import nn\nfrom torch import distributed as dist\nfrom torch import multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.optim import Adam\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torch.utils.data.distributed import DistributedSampler\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.utils import make_grid\nfrom torchvision.transforms.functional import center_crop\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as FT\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nfrom typing import Tuple, Optional, List\nfrom torch import Tensor\nfrom tqdm import tqdm\nfrom torchvision.models.mobilenetv3 import MobileNetV3, InvertedResidualConfig\nfrom torchvision.transforms.functional import normalize\nimport easing_functions as ef\n\n\nDATA_PATHS = {\n\n    'videomatte': {\n        'train': './VideoMatte240K_JPEG_SD/train',\n        'valid': './VideoMatte240K_JPEG_SD/test',\n    },\n    'background_images': {\n            'train': './Backgrounds/train',\n            'valid': './Backgrounds/valid',\n    },\n\n}\n\ndef matting_loss(pred_fgr, pred_pha, true_fgr, true_pha):\n    loss = dict()\n    # Alpha losses\n    loss['pha_l1'] = F.l1_loss(pred_pha, true_pha)\n    loss['pha_laplacian'] = laplacian_loss(pred_pha.flatten(0, 1), true_pha.flatten(0, 1))\n    loss['pha_coherence'] = F.mse_loss(pred_pha[:, 1:] - pred_pha[:, :-1],\n                                       true_pha[:, 1:] - true_pha[:, :-1]) * 5\n    # Foreground losses\n    true_msk = true_pha.gt(0)\n    pred_fgr = pred_fgr * true_msk\n    true_fgr = true_fgr * true_msk\n    loss['fgr_l1'] = F.l1_loss(pred_fgr, true_fgr)\n    loss['fgr_coherence'] = F.mse_loss(pred_fgr[:, 1:] - pred_fgr[:, :-1],\n                                       true_fgr[:, 1:] - true_fgr[:, :-1]) * 5\n    # Total\n    loss['total'] = loss['pha_l1'] + loss['pha_coherence'] + loss['pha_laplacian'] \\\n                  + loss['fgr_l1'] + loss['fgr_coherence']\n    return loss\n\ndef segmentation_loss(pred_seg, true_seg):\n    return F.binary_cross_entropy_with_logits(pred_seg, true_seg)\n\n\n# ----------------------------------------------------------------------------- Laplacian Loss\n\n\ndef laplacian_loss(pred, true, max_levels=5):\n    kernel = gauss_kernel(device=pred.device, dtype=pred.dtype)\n    pred_pyramid = laplacian_pyramid(pred, kernel, max_levels)\n    true_pyramid = laplacian_pyramid(true, kernel, max_levels)\n    loss = 0\n    for level in range(max_levels):\n        loss += (2 ** level) * F.l1_loss(pred_pyramid[level], true_pyramid[level])\n    return loss / max_levels\n\ndef laplacian_pyramid(img, kernel, max_levels):\n    current = img\n    pyramid = []\n    for _ in range(max_levels):\n        current = crop_to_even_size(current)\n        down = downsample(current, kernel)\n        up = upsample(down, kernel)\n        diff = current - up\n        pyramid.append(diff)\n        current = down\n    return pyramid\n\ndef gauss_kernel(device='cpu', dtype=torch.float32):\n    kernel = torch.tensor([[1,  4,  6,  4, 1],\n                           [4, 16, 24, 16, 4],\n                           [6, 24, 36, 24, 6],\n                           [4, 16, 24, 16, 4],\n                           [1,  4,  6,  4, 1]], device=device, dtype=dtype)\n    kernel /= 256\n    kernel = kernel[None, None, :, :]\n    return kernel\n\ndef gauss_convolution(img, kernel):\n    B, C, H, W = img.shape\n    img = img.reshape(B * C, 1, H, W)\n    img = F.pad(img, (2, 2, 2, 2), mode='reflect')\n    img = F.conv2d(img, kernel)\n    img = img.reshape(B, C, H, W)\n    return img\n\ndef downsample(img, kernel):\n    img = gauss_convolution(img, kernel)\n    img = img[:, :, ::2, ::2]\n    return img\n\ndef upsample(img, kernel):\n    B, C, H, W = img.shape\n    out = torch.zeros((B, C, H * 2, W * 2), device=img.device, dtype=img.dtype)\n    out[:, :, ::2, ::2] = img * 4\n    out = gauss_convolution(out, kernel)\n    return out\n\ndef crop_to_even_size(img):\n    H, W = img.shape[2:]\n    H = H - H % 2\n    W = W - W % 2\n    return img[:, :, :H, :W]\n\nclass MotionAugmentation:\n    def __init__(self,\n                 size,\n                 prob_fgr_affine,\n                 prob_bgr_affine,\n                 prob_noise,\n                 prob_color_jitter,\n                 prob_grayscale,\n                 prob_sharpness,\n                 prob_blur,\n                 prob_hflip,\n                 prob_pause,\n                 static_affine=True,\n                 aspect_ratio_range=(0.9, 1.1)):\n        self.size = size\n        self.prob_fgr_affine = prob_fgr_affine\n        self.prob_bgr_affine = prob_bgr_affine\n        self.prob_noise = prob_noise\n        self.prob_color_jitter = prob_color_jitter\n        self.prob_grayscale = prob_grayscale\n        self.prob_sharpness = prob_sharpness\n        self.prob_blur = prob_blur\n        self.prob_hflip = prob_hflip\n        self.prob_pause = prob_pause\n        self.static_affine = static_affine\n        self.aspect_ratio_range = aspect_ratio_range\n        \n    def __call__(self, fgrs, phas, bgrs):\n        # Foreground affine\n        if random.random() < self.prob_fgr_affine:\n            fgrs, phas = self._motion_affine(fgrs, phas)\n\n        # Background affine\n        if random.random() < self.prob_bgr_affine / 2:\n            bgrs = self._motion_affine(bgrs)\n        if random.random() < self.prob_bgr_affine / 2:\n            fgrs, phas, bgrs = self._motion_affine(fgrs, phas, bgrs)\n                \n        # Still Affine\n        if self.static_affine:\n            fgrs, phas = self._static_affine(fgrs, phas, scale_ranges=(0.5, 1))\n            bgrs = self._static_affine(bgrs, scale_ranges=(1, 1.5))\n        \n        # To tensor\n        fgrs = torch.stack([  FT.to_tensor(fgr) for fgr in fgrs])\n        phas = torch.stack([  FT.to_tensor(pha) for pha in phas])\n        bgrs = torch.stack([  FT.to_tensor(bgr) for bgr in bgrs])\n        \n        # Resize\n        params = transforms.RandomResizedCrop.get_params(fgrs, scale=(1, 1), ratio=self.aspect_ratio_range)\n        fgrs =   FT.resized_crop(fgrs, *params, self.size, interpolation=  FT.InterpolationMode.BILINEAR)\n        phas =   FT.resized_crop(phas, *params, self.size, interpolation=  FT.InterpolationMode.BILINEAR)\n        params = transforms.RandomResizedCrop.get_params(bgrs, scale=(1, 1), ratio=self.aspect_ratio_range)\n        bgrs =   FT.resized_crop(bgrs, *params, self.size, interpolation=  FT.InterpolationMode.BILINEAR)\n\n        # Horizontal flip\n        if random.random() < self.prob_hflip:\n            fgrs =   FT.hflip(fgrs)\n            phas =   FT.hflip(phas)\n        if random.random() < self.prob_hflip:\n            bgrs =   FT.hflip(bgrs)\n\n        # Noise\n        if random.random() < self.prob_noise:\n            fgrs, bgrs = self._motion_noise(fgrs, bgrs)\n        \n        # Color jitter\n        if random.random() < self.prob_color_jitter:\n            fgrs = self._motion_color_jitter(fgrs)\n        if random.random() < self.prob_color_jitter:\n            bgrs = self._motion_color_jitter(bgrs)\n            \n        # Grayscale\n        if random.random() < self.prob_grayscale:\n            fgrs =   FT.rgb_to_grayscale(fgrs, num_output_channels=3).contiguous()\n            bgrs =   FT.rgb_to_grayscale(bgrs, num_output_channels=3).contiguous()\n            \n        # Sharpen\n        if random.random() < self.prob_sharpness:\n            sharpness = random.random() * 8\n            fgrs =   FT.adjust_sharpness(fgrs, sharpness)\n            phas =   FT.adjust_sharpness(phas, sharpness)\n            bgrs =   FT.adjust_sharpness(bgrs, sharpness)\n        \n        # Blur\n        if random.random() < self.prob_blur / 3:\n            fgrs, phas = self._motion_blur(fgrs, phas)\n        if random.random() < self.prob_blur / 3:\n            bgrs = self._motion_blur(bgrs)\n        if random.random() < self.prob_blur / 3:\n            fgrs, phas, bgrs = self._motion_blur(fgrs, phas, bgrs)\n\n        # Pause\n        if random.random() < self.prob_pause:\n            fgrs, phas, bgrs = self._motion_pause(fgrs, phas, bgrs)\n        \n        return fgrs, phas, bgrs\n    \n    def _static_affine(self, *imgs, scale_ranges):\n        params = transforms.RandomAffine.get_params(\n            degrees=(-10, 10), translate=(0.1, 0.1), scale_ranges=scale_ranges,\n            shears=(-5, 5), img_size=imgs[0][0].size)\n        imgs = [[  FT.affine(t, *params,   FT.InterpolationMode.BILINEAR) for t in img] for img in imgs]\n        return imgs if len(imgs) > 1 else imgs[0] \n    \n    def _motion_affine(self, *imgs):\n        config = dict(degrees=(-10, 10), translate=(0.1, 0.1),\n                      scale_ranges=(0.9, 1.1), shears=(-5, 5), img_size=imgs[0][0].size)\n        angleA, (transXA, transYA), scaleA, (shearXA, shearYA) = transforms.RandomAffine.get_params(**config)\n        angleB, (transXB, transYB), scaleB, (shearXB, shearYB) = transforms.RandomAffine.get_params(**config)\n        \n        T = len(imgs[0])\n        easing = random_easing_fn()\n        for t in range(T):\n            percentage = easing(t / (T - 1))\n            angle = lerp(angleA, angleB, percentage)\n            transX = lerp(transXA, transXB, percentage)\n            transY = lerp(transYA, transYB, percentage)\n            scale = lerp(scaleA, scaleB, percentage)\n            shearX = lerp(shearXA, shearXB, percentage)\n            shearY = lerp(shearYA, shearYB, percentage)\n            for img in imgs:\n                img[t] =   FT.affine(img[t], angle, (transX, transY), scale, (shearX, shearY),   FT.InterpolationMode.BILINEAR)\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_noise(self, *imgs):\n        grain_size = random.random() * 3 + 1 # range 1 ~ 4\n        monochrome = random.random() < 0.5\n        for img in imgs:\n            T, C, H, W = img.shape\n            noise = torch.randn((T, 1 if monochrome else C, round(H / grain_size), round(W / grain_size)))\n            noise.mul_(random.random() * 0.2 / grain_size)\n            if grain_size != 1:\n                noise =   FT.resize(noise, (H, W))\n            img.add_(noise).clamp_(0, 1)\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_color_jitter(self, *imgs):\n        brightnessA, brightnessB, contrastA, contrastB, saturationA, saturationB, hueA, hueB \\\n            = torch.randn(8).mul(0.1).tolist()\n        strength = random.random() * 0.2\n        easing = random_easing_fn()\n        T = len(imgs[0])\n        for t in range(T):\n            percentage = easing(t / (T - 1)) * strength\n            for img in imgs:\n                img[t] =   FT.adjust_brightness(img[t], max(1 + lerp(brightnessA, brightnessB, percentage), 0.1))\n                img[t] =   FT.adjust_contrast(img[t], max(1 + lerp(contrastA, contrastB, percentage), 0.1))\n                img[t] =   FT.adjust_saturation(img[t], max(1 + lerp(brightnessA, brightnessB, percentage), 0.1))\n                img[t] =   FT.adjust_hue(img[t], min(0.5, max(-0.5, lerp(hueA, hueB, percentage) * 0.1)))\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_blur(self, *imgs):\n        blurA = random.random() * 10\n        blurB = random.random() * 10\n\n        T = len(imgs[0])\n        easing = random_easing_fn()\n        for t in range(T):\n            percentage = easing(t / (T - 1))\n            blur = max(lerp(blurA, blurB, percentage), 0)\n            if blur != 0:\n                kernel_size = int(blur * 2)\n                if kernel_size % 2 == 0:\n                    kernel_size += 1 # Make kernel_size odd\n                for img in imgs:\n                    img[t] =   FT.gaussian_blur(img[t], kernel_size, sigma=blur)\n    \n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_pause(self, *imgs):\n        T = len(imgs[0])\n        pause_frame = random.choice(range(T - 1))\n        pause_length = random.choice(range(T - pause_frame))\n        for img in imgs:\n            img[pause_frame + 1 : pause_frame + pause_length] = img[pause_frame]\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n\ndef lerp(a, b, percentage):\n    return a * (1 - percentage) + b * percentage\n\n\n\nclass Step: # Custom easing function for sudden change.\n    def __call__(self, value):\n        return 0 if value < 0.5 else 1\n\n\nclass TrainFrameSampler:\n    def __init__(self, speed=[0.5, 1, 2, 3, 4, 5]):\n        self.speed = speed\n    \n    def __call__(self, seq_length):\n        frames = list(range(seq_length))\n        \n        # Speed up\n        speed = random.choice(self.speed)\n        frames = [int(f * speed) for f in frames]\n        \n        # Shift\n        shift = random.choice(range(seq_length))\n        frames = [f + shift for f in frames]\n        \n        # Reverse\n        if random.random() < 0.5:\n            frames = frames[::-1]\n\n        return frames\n    \nclass ValidFrameSampler:\n    def __call__(self, seq_length):\n        return range(seq_length)\n    \nclass VideoMatteDataset(Dataset):\n    def __init__(self,\n                 videomatte_dir,\n                 background_image_dir,\n                 size,\n                 seq_length,\n                 seq_sampler,\n                 transform=None):\n        self.background_image_dir = background_image_dir\n        self.background_image_files = os.listdir(background_image_dir)\n        self.videomatte_dir = videomatte_dir\n        self.videomatte_clips = sorted(os.listdir(os.path.join(videomatte_dir, 'fgr')))\n        self.videomatte_frames = [sorted(os.listdir(os.path.join(videomatte_dir, 'fgr', clip))) \n                                  for clip in self.videomatte_clips]\n        self.videomatte_idx = [(clip_idx, frame_idx) \n                               for clip_idx in range(len(self.videomatte_clips)) \n                               for frame_idx in range(0, len(self.videomatte_frames[clip_idx]), seq_length)]\n        self.size = size\n        self.seq_length = seq_length\n        self.seq_sampler = seq_sampler\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.videomatte_idx)\n    \n    def __getitem__(self, idx):\n        bgrs = self._get_random_image_background()\n\n        \n        fgrs, phas = self._get_videomatte(idx)\n        \n        if self.transform is not None:\n            return self.transform(fgrs, phas, bgrs)\n        \n        return fgrs, phas, bgrs\n    \n    def _get_random_image_background(self):\n        with Image.open(os.path.join(self.background_image_dir, random.choice(self.background_image_files))) as bgr:\n            bgr = self._downsample_if_needed(bgr.convert('RGB'))\n        bgrs = [bgr] * self.seq_length\n        return bgrs\n    \n    \n    def _get_videomatte(self, idx):\n        clip_idx, frame_idx = self.videomatte_idx[idx]\n        clip = self.videomatte_clips[clip_idx]\n        frame_count = len(self.videomatte_frames[clip_idx])\n        fgrs, phas = [], []\n        for i in self.seq_sampler(self.seq_length):\n            frame = self.videomatte_frames[clip_idx][(frame_idx + i) % frame_count]\n            with Image.open(os.path.join(self.videomatte_dir, 'fgr', clip, frame)) as fgr, \\\n                 Image.open(os.path.join(self.videomatte_dir, 'pha', clip, frame)) as pha:\n                    fgr = self._downsample_if_needed(fgr.convert('RGB'))\n                    pha = self._downsample_if_needed(pha.convert('L'))\n            fgrs.append(fgr)\n            phas.append(pha)\n        return fgrs, phas\n    \n    def _downsample_if_needed(self, img):\n        w, h = img.size\n        if min(w, h) > self.size:\n            scale = self.size / min(w, h)\n            w = int(scale * w)\n            h = int(scale * h)\n            img = img.resize((w, h))\n        return img\n\nclass VideoMatteTrainAugmentation(MotionAugmentation):\n    def __init__(self, size):\n        super().__init__(\n            size=size,\n            prob_fgr_affine=0.3,\n            prob_bgr_affine=0.3,\n            prob_noise=0.1,\n            prob_color_jitter=0.3,\n            prob_grayscale=0.02,\n            prob_sharpness=0.1,\n            prob_blur=0.02,\n            prob_hflip=0.5,\n            prob_pause=0.03,\n        )\n\nclass VideoMatteValidAugmentation(MotionAugmentation):\n    def __init__(self, size):\n        super().__init__(\n            size=size,\n            prob_fgr_affine=0,\n            prob_bgr_affine=0,\n            prob_noise=0,\n            prob_color_jitter=0,\n            prob_grayscale=0,\n            prob_sharpness=0,\n            prob_blur=0,\n            prob_hflip=0,\n            prob_pause=0,\n        )\n\nclass MotionAugmentation:\n    def __init__(self,\n                 size,\n                 prob_fgr_affine,\n                 prob_bgr_affine,\n                 prob_noise,\n                 prob_color_jitter,\n                 prob_grayscale,\n                 prob_sharpness,\n                 prob_blur,\n                 prob_hflip,\n                 prob_pause,\n                 static_affine=True,\n                 aspect_ratio_range=(0.9, 1.1)):\n        self.size = size\n        self.prob_fgr_affine = prob_fgr_affine\n        self.prob_bgr_affine = prob_bgr_affine\n        self.prob_noise = prob_noise\n        self.prob_color_jitter = prob_color_jitter\n        self.prob_grayscale = prob_grayscale\n        self.prob_sharpness = prob_sharpness\n        self.prob_blur = prob_blur\n        self.prob_hflip = prob_hflip\n        self.prob_pause = prob_pause\n        self.static_affine = static_affine\n        self.aspect_ratio_range = aspect_ratio_range\n        \n    def __call__(self, fgrs, phas, bgrs):\n        # Foreground affine\n        if random.random() < self.prob_fgr_affine:\n            fgrs, phas = self._motion_affine(fgrs, phas)\n\n        # Background affine\n        if random.random() < self.prob_bgr_affine / 2:\n            bgrs = self._motion_affine(bgrs)\n        if random.random() < self.prob_bgr_affine / 2:\n            fgrs, phas, bgrs = self._motion_affine(fgrs, phas, bgrs)\n                \n        # Still Affine\n        if self.static_affine:\n            fgrs, phas = self._static_affine(fgrs, phas, scale_ranges=(0.5, 1))\n            bgrs = self._static_affine(bgrs, scale_ranges=(1, 1.5))\n        \n        # To tensor\n        fgrs = torch.stack([FT.to_tensor(fgr) for fgr in fgrs])\n        phas = torch.stack([FT.to_tensor(pha) for pha in phas])\n        bgrs = torch.stack([FT.to_tensor(bgr) for bgr in bgrs])\n        \n        # Resize\n        params = transforms.RandomResizedCrop.get_params(fgrs, scale=(1, 1), ratio=self.aspect_ratio_range)\n        fgrs = FT.resized_crop(fgrs, *params, self.size, interpolation=FT.InterpolationMode.BILINEAR)\n        phas = FT.resized_crop(phas, *params, self.size, interpolation=FT.InterpolationMode.BILINEAR)\n        params = transforms.RandomResizedCrop.get_params(bgrs, scale=(1, 1), ratio=self.aspect_ratio_range)\n        bgrs = FT.resized_crop(bgrs, *params, self.size, interpolation=FT.InterpolationMode.BILINEAR)\n\n        # Horizontal flip\n        if random.random() < self.prob_hflip:\n            fgrs = FT.hflip(fgrs)\n            phas = FT.hflip(phas)\n        if random.random() < self.prob_hflip:\n            bgrs = FT.hflip(bgrs)\n\n        # Noise\n        if random.random() < self.prob_noise:\n            fgrs, bgrs = self._motion_noise(fgrs, bgrs)\n        \n        # Color jitter\n        if random.random() < self.prob_color_jitter:\n            fgrs = self._motion_color_jitter(fgrs)\n        if random.random() < self.prob_color_jitter:\n            bgrs = self._motion_color_jitter(bgrs)\n            \n        # Grayscale\n        if random.random() < self.prob_grayscale:\n            fgrs = FT.rgb_to_grayscale(fgrs, num_output_channels=3).contiguous()\n            bgrs = FT.rgb_to_grayscale(bgrs, num_output_channels=3).contiguous()\n            \n        # Sharpen\n        if random.random() < self.prob_sharpness:\n            sharpness = random.random() * 8\n            fgrs = FT.adjust_sharpness(fgrs, sharpness)\n            phas = FT.adjust_sharpness(phas, sharpness)\n            bgrs = FT.adjust_sharpness(bgrs, sharpness)\n        \n        # Blur\n        if random.random() < self.prob_blur / 3:\n            fgrs, phas = self._motion_blur(fgrs, phas)\n        if random.random() < self.prob_blur / 3:\n            bgrs = self._motion_blur(bgrs)\n        if random.random() < self.prob_blur / 3:\n            fgrs, phas, bgrs = self._motion_blur(fgrs, phas, bgrs)\n\n        # Pause\n        if random.random() < self.prob_pause:\n            fgrs, phas, bgrs = self._motion_pause(fgrs, phas, bgrs)\n        \n        return fgrs, phas, bgrs\n    \n    def _static_affine(self, *imgs, scale_ranges):\n        params = transforms.RandomAffine.get_params(\n            degrees=(-10, 10), translate=(0.1, 0.1), scale_ranges=scale_ranges,\n            shears=(-5, 5), img_size=imgs[0][0].size)\n        imgs = [[FT.affine(t, *params, FT.InterpolationMode.BILINEAR) for t in img] for img in imgs]\n        return imgs if len(imgs) > 1 else imgs[0] \n    \n    def _motion_affine(self, *imgs):\n        config = dict(degrees=(-10, 10), translate=(0.1, 0.1),\n                      scale_ranges=(0.9, 1.1), shears=(-5, 5), img_size=imgs[0][0].size)\n        angleA, (transXA, transYA), scaleA, (shearXA, shearYA) = transforms.RandomAffine.get_params(**config)\n        angleB, (transXB, transYB), scaleB, (shearXB, shearYB) = transforms.RandomAffine.get_params(**config)\n        \n        T = len(imgs[0])\n        easing = random_easing_fn()\n        for t in range(T):\n            percentage = easing(t / (T - 1))\n            angle = lerp(angleA, angleB, percentage)\n            transX = lerp(transXA, transXB, percentage)\n            transY = lerp(transYA, transYB, percentage)\n            scale = lerp(scaleA, scaleB, percentage)\n            shearX = lerp(shearXA, shearXB, percentage)\n            shearY = lerp(shearYA, shearYB, percentage)\n            for img in imgs:\n                img[t] = FT.affine(img[t], angle, (transX, transY), scale, (shearX, shearY), FT.InterpolationMode.BILINEAR)\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_noise(self, *imgs):\n        grain_size = random.random() * 3 + 1 # range 1 ~ 4\n        monochrome = random.random() < 0.5\n        for img in imgs:\n            T, C, H, W = img.shape\n            noise = torch.randn((T, 1 if monochrome else C, round(H / grain_size), round(W / grain_size)))\n            noise.mul_(random.random() * 0.2 / grain_size)\n            if grain_size != 1:\n                noise = FT.resize(noise, (H, W))\n            img.add_(noise).clamp_(0, 1)\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_color_jitter(self, *imgs):\n        brightnessA, brightnessB, contrastA, contrastB, saturationA, saturationB, hueA, hueB \\\n            = torch.randn(8).mul(0.1).tolist()\n        strength = random.random() * 0.2\n        easing = random_easing_fn()\n        T = len(imgs[0])\n        for t in range(T):\n            percentage = easing(t / (T - 1)) * strength\n            for img in imgs:\n                img[t] = FT.adjust_brightness(img[t], max(1 + lerp(brightnessA, brightnessB, percentage), 0.1))\n                img[t] = FT.adjust_contrast(img[t], max(1 + lerp(contrastA, contrastB, percentage), 0.1))\n                img[t] = FT.adjust_saturation(img[t], max(1 + lerp(brightnessA, brightnessB, percentage), 0.1))\n                img[t] = FT.adjust_hue(img[t], min(0.5, max(-0.5, lerp(hueA, hueB, percentage) * 0.1)))\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_blur(self, *imgs):\n        blurA = random.random() * 10\n        blurB = random.random() * 10\n\n        T = len(imgs[0])\n        easing = random_easing_fn()\n        for t in range(T):\n            percentage = easing(t / (T - 1))\n            blur = max(lerp(blurA, blurB, percentage), 0)\n            if blur != 0:\n                kernel_size = int(blur * 2)\n                if kernel_size % 2 == 0:\n                    kernel_size += 1 # Make kernel_size odd\n                for img in imgs:\n                    img[t] = FT.gaussian_blur(img[t], kernel_size, sigma=blur)\n    \n        return imgs if len(imgs) > 1 else imgs[0]\n    \n    def _motion_pause(self, *imgs):\n        T = len(imgs[0])\n        pause_frame = random.choice(range(T - 1))\n        pause_length = random.choice(range(T - pause_frame))\n        for img in imgs:\n            img[pause_frame + 1 : pause_frame + pause_length] = img[pause_frame]\n        return imgs if len(imgs) > 1 else imgs[0]\n    \n\ndef lerp(a, b, percentage):\n    return a * (1 - percentage) + b * percentage\n\n\ndef random_easing_fn():\n    if random.random() < 0.2:\n        return ef.LinearInOut()\n    else:\n        return random.choice([\n            ef.BackEaseIn,\n            ef.BackEaseOut,\n            ef.BackEaseInOut,\n            ef.BounceEaseIn,\n            ef.BounceEaseOut,\n            ef.BounceEaseInOut,\n            ef.CircularEaseIn,\n            ef.CircularEaseOut,\n            ef.CircularEaseInOut,\n            ef.CubicEaseIn,\n            ef.CubicEaseOut,\n            ef.CubicEaseInOut,\n            ef.ExponentialEaseIn,\n            ef.ExponentialEaseOut,\n            ef.ExponentialEaseInOut,\n            ef.ElasticEaseIn,\n            ef.ElasticEaseOut,\n            ef.ElasticEaseInOut,\n            ef.QuadEaseIn,\n            ef.QuadEaseOut,\n            ef.QuadEaseInOut,\n            ef.QuarticEaseIn,\n            ef.QuarticEaseOut,\n            ef.QuarticEaseInOut,\n            ef.QuinticEaseIn,\n            ef.QuinticEaseOut,\n            ef.QuinticEaseInOut,\n            ef.SineEaseIn,\n            ef.SineEaseOut,\n            ef.SineEaseInOut,\n            Step,\n        ])()\n\nclass Step: # Custom easing function for sudden change.\n    def __call__(self, value):\n        return 0 if value < 0.5 else 1\n\n\n# ---------------------------- Frame Sampler ----------------------------\n\n\nclass TrainFrameSampler:\n    def __init__(self, speed=[0.5, 1, 2, 3, 4, 5]):\n        self.speed = speed\n    \n    def __call__(self, seq_length):\n        frames = list(range(seq_length))\n        \n        # Speed up\n        speed = random.choice(self.speed)\n        frames = [int(FT * speed) for FT in frames]\n        \n        # Shift\n        shift = random.choice(range(seq_length))\n        frames = [FT + shift for FT in frames]\n        \n        # Reverse\n        if random.random() < 0.5:\n            frames = frames[::-1]\n\n        return frames\n    \nclass ValidFrameSampler:\n    def __call__(self, seq_length):\n        return range(seq_length)\n\n\nclass FastGuidedFilterRefiner(nn.Module):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        self.guilded_filter = FastGuidedFilter(1)\n    \n    def forward_single_frame(self, fine_src, base_src, base_fgr, base_pha):\n        fine_src_gray = fine_src.mean(1, keepdim=True)\n        base_src_gray = base_src.mean(1, keepdim=True)\n        \n        fgr, pha = self.guilded_filter(\n            torch.cat([base_src, base_src_gray], dim=1),\n            torch.cat([base_fgr, base_pha], dim=1),\n            torch.cat([fine_src, fine_src_gray], dim=1)).split([3, 1], dim=1)\n        \n        return fgr, pha\n    \n    def forward_time_series(self, fine_src, base_src, base_fgr, base_pha):\n        B, T = fine_src.shape[:2]\n        fgr, pha = self.forward_single_frame(\n            fine_src.flatten(0, 1),\n            base_src.flatten(0, 1),\n            base_fgr.flatten(0, 1),\n            base_pha.flatten(0, 1))\n        fgr = fgr.unflatten(0, (B, T))\n        pha = pha.unflatten(0, (B, T))\n        return fgr, pha\n    \n    def forward(self, fine_src, base_src, base_fgr, base_pha, base_hid):\n        if fine_src.ndim == 5:\n            return self.forward_time_series(fine_src, base_src, base_fgr, base_pha)\n        else:\n            return self.forward_single_frame(fine_src, base_src, base_fgr, base_pha)\n\n\nclass FastGuidedFilter(nn.Module):\n    def __init__(self, r: int, eps: float = 1e-5):\n        super().__init__()\n        self.r = r\n        self.eps = eps\n        self.boxfilter = BoxFilter(r)\n\n    def forward(self, lr_x, lr_y, hr_x):\n        mean_x = self.boxfilter(lr_x)\n        mean_y = self.boxfilter(lr_y)\n        cov_xy = self.boxfilter(lr_x * lr_y) - mean_x * mean_y\n        var_x = self.boxfilter(lr_x * lr_x) - mean_x * mean_x\n        A = cov_xy / (var_x + self.eps)\n        b = mean_y - A * mean_x\n        A = F.interpolate(A, hr_x.shape[2:], mode='bilinear', align_corners=False)\n        b = F.interpolate(b, hr_x.shape[2:], mode='bilinear', align_corners=False)\n        return A * hr_x + b\n\n\nclass BoxFilter(nn.Module):\n    def __init__(self, r):\n        super(BoxFilter, self).__init__()\n        self.r = r\n\n    def forward(self, x):\n        # Note: The original implementation at <https://github.com/wuhuikai/DeepGuidedFilter/>\n        #       uses faster box blur. However, it may not be friendly for ONNX export.\n        #       We are switching to use simple convolution for box blur.\n        kernel_size = 2 * self.r + 1\n        kernel_x = torch.full((x.data.shape[1], 1, 1, kernel_size), 1 / kernel_size, device=x.device, dtype=x.dtype)\n        kernel_y = torch.full((x.data.shape[1], 1, kernel_size, 1), 1 / kernel_size, device=x.device, dtype=x.dtype)\n        x = F.conv2d(x, kernel_x, padding=(0, self.r), groups=x.data.shape[1])\n        x = F.conv2d(x, kernel_y, padding=(self.r, 0), groups=x.data.shape[1])\n        return x\n\nclass RecurrentDecoder(nn.Module):\n    def __init__(self, feature_channels, decoder_channels):\n        super().__init__()\n        self.avgpool = AvgPool()\n        self.decode4 = BottleneckBlock(feature_channels[3])\n        self.decode3 = UpsamplingBlock(feature_channels[3], feature_channels[2], 3, decoder_channels[0])\n        self.decode2 = UpsamplingBlock(decoder_channels[0], feature_channels[1], 3, decoder_channels[1])\n        self.decode1 = UpsamplingBlock(decoder_channels[1], feature_channels[0], 3, decoder_channels[2])\n        self.decode0 = OutputBlock(decoder_channels[2], 3, decoder_channels[3])\n\n    def forward(self,\n                s0: Tensor, f1: Tensor, f2: Tensor, f3: Tensor, f4: Tensor,\n                r1: Optional[Tensor], r2: Optional[Tensor],\n                r3: Optional[Tensor], r4: Optional[Tensor]):\n        s1, s2, s3 = self.avgpool(s0)\n        x4, r4 = self.decode4(f4, r4)\n        x3, r3 = self.decode3(x4, f3, s3, r3)\n        x2, r2 = self.decode2(x3, f2, s2, r2)\n        x1, r1 = self.decode1(x2, f1, s1, r1)\n        x0 = self.decode0(x1, s0)\n        return x0, r1, r2, r3, r4\n    \n\nclass AvgPool(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.avgpool = nn.AvgPool2d(2, 2, count_include_pad=False, ceil_mode=True)\n        \n    def forward_single_frame(self, s0):\n        s1 = self.avgpool(s0)\n        s2 = self.avgpool(s1)\n        s3 = self.avgpool(s2)\n        return s1, s2, s3\n    \n    def forward_time_series(self, s0):\n        B, T = s0.shape[:2]\n        s0 = s0.flatten(0, 1)\n        s1, s2, s3 = self.forward_single_frame(s0)\n        s1 = s1.unflatten(0, (B, T))\n        s2 = s2.unflatten(0, (B, T))\n        s3 = s3.unflatten(0, (B, T))\n        return s1, s2, s3\n    \n    def forward(self, s0):\n        if s0.ndim == 5:\n            return self.forward_time_series(s0)\n        else:\n            return self.forward_single_frame(s0)\n\n\nclass BottleneckBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.channels = channels\n        self.gru = ConvGRU(channels // 2)\n        \n    def forward(self, x, r: Optional[Tensor]):\n        a, b = x.split(self.channels // 2, dim=-3)\n        b, r = self.gru(b, r)\n        x = torch.cat([a, b], dim=-3)\n        return x, r\n\n    \nclass UpsamplingBlock(nn.Module):\n    def __init__(self, in_channels, skip_channels, src_channels, out_channels):\n        super().__init__()\n        self.out_channels = out_channels\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels + skip_channels + src_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n        )\n        self.gru = ConvGRU(out_channels // 2)\n\n    def forward_single_frame(self, x, f, s, r: Optional[Tensor]):\n        x = self.upsample(x)\n        x = x[:, :, :s.size(2), :s.size(3)]\n        x = torch.cat([x, f, s], dim=1)\n        x = self.conv(x)\n        a, b = x.split(self.out_channels // 2, dim=1)\n        b, r = self.gru(b, r)\n        x = torch.cat([a, b], dim=1)\n        return x, r\n    \n    def forward_time_series(self, x, f, s, r: Optional[Tensor]):\n        B, T, _, H, W = s.shape\n        x = x.flatten(0, 1)\n        f = f.flatten(0, 1)\n        s = s.flatten(0, 1)\n        x = self.upsample(x)\n        x = x[:, :, :H, :W]\n        x = torch.cat([x, f, s], dim=1)\n        x = self.conv(x)\n        x = x.unflatten(0, (B, T))\n        a, b = x.split(self.out_channels // 2, dim=2)\n        b, r = self.gru(b, r)\n        x = torch.cat([a, b], dim=2)\n        return x, r\n    \n    def forward(self, x, f, s, r: Optional[Tensor]):\n        if x.ndim == 5:\n            return self.forward_time_series(x, f, s, r)\n        else:\n            return self.forward_single_frame(x, f, s, r)\n\n\nclass OutputBlock(nn.Module):\n    def __init__(self, in_channels, src_channels, out_channels):\n        super().__init__()\n        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels + src_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True),\n        )\n        \n    def forward_single_frame(self, x, s):\n        x = self.upsample(x)\n        x = x[:, :, :s.size(2), :s.size(3)]\n        x = torch.cat([x, s], dim=1)\n        x = self.conv(x)\n        return x\n    \n    def forward_time_series(self, x, s):\n        B, T, _, H, W = s.shape\n        x = x.flatten(0, 1)\n        s = s.flatten(0, 1)\n        x = self.upsample(x)\n        x = x[:, :, :H, :W]\n        x = torch.cat([x, s], dim=1)\n        x = self.conv(x)\n        x = x.unflatten(0, (B, T))\n        return x\n    \n    def forward(self, x, s):\n        if x.ndim == 5:\n            return self.forward_time_series(x, s)\n        else:\n            return self.forward_single_frame(x, s)\n\n\nclass ConvGRU(nn.Module):\n    def __init__(self,\n                 channels: int,\n                 kernel_size: int = 3,\n                 padding: int = 1):\n        super().__init__()\n        self.channels = channels\n        self.ih = nn.Sequential(\n            nn.Conv2d(channels * 2, channels * 2, kernel_size, padding=padding),\n            nn.Sigmoid()\n        )\n        self.hh = nn.Sequential(\n            nn.Conv2d(channels * 2, channels, kernel_size, padding=padding),\n            nn.Tanh()\n        )\n        \n    def forward_single_frame(self, x, h):\n        r, z = self.ih(torch.cat([x, h], dim=1)).split(self.channels, dim=1)\n        c = self.hh(torch.cat([x, r * h], dim=1))\n        h = (1 - z) * h + z * c\n        return h, h\n    \n    def forward_time_series(self, x, h):\n        o = []\n        for xt in x.unbind(dim=1):\n            ot, h = self.forward_single_frame(xt, h)\n            o.append(ot)\n        o = torch.stack(o, dim=1)\n        return o, h\n        \n    def forward(self, x, h: Optional[Tensor]):\n        if h is None:\n            h = torch.zeros((x.size(0), x.size(-3), x.size(-2), x.size(-1)),\n                            device=x.device, dtype=x.dtype)\n        \n        if x.ndim == 5:\n            return self.forward_time_series(x, h)\n        else:\n            return self.forward_single_frame(x, h)\n\n\nclass Projection(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, 1)\n    \n    def forward_single_frame(self, x):\n        return self.conv(x)\n    \n    def forward_time_series(self, x):\n        B, T = x.shape[:2]\n        return self.conv(x.flatten(0, 1)).unflatten(0, (B, T))\n        \n    def forward(self, x):\n        if x.ndim == 5:\n            return self.forward_time_series(x)\n        else:\n            return self.forward_single_frame(x)\n\nclass MobileNetV3LargeEncoder(MobileNetV3):\n    def __init__(self, pretrained: bool = False):\n        super().__init__(\n            inverted_residual_setting=[\n                InvertedResidualConfig( 16, 3,  16,  16, False, \"RE\", 1, 1, 1),\n                InvertedResidualConfig( 16, 3,  64,  24, False, \"RE\", 2, 1, 1),  # C1\n                InvertedResidualConfig( 24, 3,  72,  24, False, \"RE\", 1, 1, 1),\n                InvertedResidualConfig( 24, 5,  72,  40,  True, \"RE\", 2, 1, 1),  # C2\n                InvertedResidualConfig( 40, 5, 120,  40,  True, \"RE\", 1, 1, 1),\n                InvertedResidualConfig( 40, 5, 120,  40,  True, \"RE\", 1, 1, 1),\n                InvertedResidualConfig( 40, 3, 240,  80, False, \"HS\", 2, 1, 1),  # C3\n                InvertedResidualConfig( 80, 3, 200,  80, False, \"HS\", 1, 1, 1),\n                InvertedResidualConfig( 80, 3, 184,  80, False, \"HS\", 1, 1, 1),\n                InvertedResidualConfig( 80, 3, 184,  80, False, \"HS\", 1, 1, 1),\n                InvertedResidualConfig( 80, 3, 480, 112,  True, \"HS\", 1, 1, 1),\n                InvertedResidualConfig(112, 3, 672, 112,  True, \"HS\", 1, 1, 1),\n                InvertedResidualConfig(112, 5, 672, 160,  True, \"HS\", 2, 2, 1),  # C4\n                InvertedResidualConfig(160, 5, 960, 160,  True, \"HS\", 1, 2, 1),\n                InvertedResidualConfig(160, 5, 960, 160,  True, \"HS\", 1, 2, 1),\n            ],\n            last_channel=1280\n        )\n        \n        if pretrained:\n            self.load_state_dict(torch.hub.load_state_dict_from_url(\n                'https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth'))\n\n        del self.avgpool\n        del self.classifier\n        \n    def forward_single_frame(self, x):\n        x = normalize(x, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        \n        x = self.features[0](x)\n        x = self.features[1](x)\n        f1 = x\n        x = self.features[2](x)\n        x = self.features[3](x)\n        f2 = x\n        x = self.features[4](x)\n        x = self.features[5](x)\n        x = self.features[6](x)\n        f3 = x\n        x = self.features[7](x)\n        x = self.features[8](x)\n        x = self.features[9](x)\n        x = self.features[10](x)\n        x = self.features[11](x)\n        x = self.features[12](x)\n        x = self.features[13](x)\n        x = self.features[14](x)\n        x = self.features[15](x)\n        x = self.features[16](x)\n        f4 = x\n        return [f1, f2, f3, f4]\n    \n    def forward_time_series(self, x):\n        B, T = x.shape[:2]\n        features = self.forward_single_frame(x.flatten(0, 1))\n        features = [f.unflatten(0, (B, T)) for f in features]\n        return features\n\n    def forward(self, x):\n        if x.ndim == 5:\n            return self.forward_time_series(x)\n        else:\n            return self.forward_single_frame(x)\n        \nclass LRASPP(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.aspp1 = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n        self.aspp2 = nn.Sequential(\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n            nn.Sigmoid()\n        )\n        \n    def forward_single_frame(self, x):\n        return self.aspp1(x) * self.aspp2(x)\n    \n    def forward_time_series(self, x):\n        B, T = x.shape[:2]\n        x = self.forward_single_frame(x.flatten(0, 1)).unflatten(0, (B, T))\n        return x\n    \n    def forward(self, x):\n        if x.ndim == 5:\n            return self.forward_time_series(x)\n        else:\n            return self.forward_single_frame(x)\n        \nclass MattingNetwork(nn.Module):\n    def __init__(self,\n                 variant: str = 'mobilenetv3',\n                 refiner: str = 'deep_guided_filter',\n                 pretrained_backbone: bool = False):\n        super().__init__()\n        assert variant in ['mobilenetv3', 'resnet50']\n        assert refiner in ['fast_guided_filter', 'deep_guided_filter']\n        \n\n        self.backbone = MobileNetV3LargeEncoder(pretrained_backbone)\n        self.aspp = LRASPP(960, 128)\n        self.decoder = RecurrentDecoder([16, 24, 40, 128], [80, 40, 32, 16])\n\n            \n        self.project_mat = Projection(16, 4)\n        self.project_seg = Projection(16, 1)\n\n\n        self.refiner = FastGuidedFilterRefiner()\n        \n    def forward(self,\n                src: Tensor,\n                r1: Optional[Tensor] = None,\n                r2: Optional[Tensor] = None,\n                r3: Optional[Tensor] = None,\n                r4: Optional[Tensor] = None,\n                downsample_ratio: float = 1,\n                segmentation_pass: bool = False):\n        \n        if downsample_ratio != 1:\n            src_sm = self._interpolate(src, scale_factor=downsample_ratio)\n        else:\n            src_sm = src\n        \n        f1, f2, f3, f4 = self.backbone(src_sm)\n        f4 = self.aspp(f4)\n        hid, *rec = self.decoder(src_sm, f1, f2, f3, f4, r1, r2, r3, r4)\n        \n        if not segmentation_pass:\n            fgr_residual, pha = self.project_mat(hid).split([3, 1], dim=-3)\n            if downsample_ratio != 1:\n                fgr_residual, pha = self.refiner(src, src_sm, fgr_residual, pha, hid)\n            fgr = fgr_residual + src\n            fgr = fgr.clamp(0., 1.)\n            pha = pha.clamp(0., 1.)\n            return [fgr, pha, *rec]\n        else:\n            seg = self.project_seg(hid)\n            return [seg, *rec]\n\n    def _interpolate(self, x: Tensor, scale_factor: float):\n        if x.ndim == 5:\n            B, T = x.shape[:2]\n            x = F.interpolate(x.flatten(0, 1), scale_factor=scale_factor,\n                mode='bilinear', align_corners=False, recompute_scale_factor=False)\n            x = x.unflatten(0, (B, T))\n        else:\n            x = F.interpolate(x, scale_factor=scale_factor,\n                mode='bilinear', align_corners=False, recompute_scale_factor=False)\n        return x\n\nclass Trainer:\n    def __init__(self, rank, world_size):\n        self.parse_args()\n        self.init_distributed(rank, world_size)\n        self.init_datasets()\n        self.init_model()\n        self.init_writer()\n        self.train()\n        self.cleanup()\n        \n    def parse_args(self):\n        parser = argparse.ArgumentParser()\n        # Model\n        parser.add_argument('--model-variant', type=str, required=True, choices=['mobilenetv3'])\n        # Matting dataset\n        parser.add_argument('--dataset', type=str, required=True, choices=['videomatte', 'imagematte'])\n        # Learning rate\n        parser.add_argument('--learning-rate-backbone', type=float, required=True)\n        parser.add_argument('--learning-rate-aspp', type=float, required=True)\n        parser.add_argument('--learning-rate-decoder', type=float, required=True)\n        parser.add_argument('--learning-rate-refiner', type=float, required=True)\n        # Training setting\n        parser.add_argument('--train-hr', action='store_true')\n        parser.add_argument('--resolution-lr', type=int, default=512)\n        parser.add_argument('--resolution-hr', type=int, default=2048)\n        parser.add_argument('--seq-length-lr', type=int, required=True)\n        parser.add_argument('--seq-length-hr', type=int, default=6)\n        parser.add_argument('--downsample-ratio', type=float, default=0.25)\n        parser.add_argument('--batch-size-per-gpu', type=int, default=1)\n        parser.add_argument('--num-workers', type=int, default=8)\n        parser.add_argument('--epoch-start', type=int, default=0)\n        parser.add_argument('--epoch-end', type=int, default=16)\n        # Tensorboard logging\n        parser.add_argument('--log-dir', type=str, required=True)\n        parser.add_argument('--log-train-loss-interval', type=int, default=20)\n        parser.add_argument('--log-train-images-interval', type=int, default=500)\n        # Checkpoint loading and saving\n        parser.add_argument('--checkpoint', type=str)\n        parser.add_argument('--checkpoint-dir', type=str, required=True)\n        parser.add_argument('--checkpoint-save-interval', type=int, default=500)\n        # Distributed\n        parser.add_argument('--distributed-addr', type=str, default='localhost')\n        parser.add_argument('--distributed-port', type=str, default='12355')\n        # Debugging\n        parser.add_argument('--disable-progress-bar', action='store_true')\n        parser.add_argument('--disable-validation', action='store_true')\n        parser.add_argument('--disable-mixed-precision', action='store_true')\n        self.args = parser.parse_args()\n        \n    def init_distributed(self, rank, world_size):\n        self.rank = rank\n        self.world_size = world_size\n        self.log('Initializing distributed')\n        os.environ['MASTER_ADDR'] = self.args.distributed_addr\n        os.environ['MASTER_PORT'] = self.args.distributed_port\n        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n    \n    def init_datasets(self):\n        self.log('Initializing matting datasets')\n        size_hr = (self.args.resolution_hr, self.args.resolution_hr)\n        size_lr = (self.args.resolution_lr, self.args.resolution_lr)\n        \n        # Matting datasets:\n        if self.args.dataset == 'videomatte':\n            self.dataset_lr_train = VideoMatteDataset(\n                videomatte_dir=DATA_PATHS['videomatte']['train'],\n                background_image_dir=DATA_PATHS['background_images']['train'],\n                size=self.args.resolution_lr,\n                seq_length=self.args.seq_length_lr,\n                seq_sampler=TrainFrameSampler(),\n                transform=VideoMatteTrainAugmentation(size_lr))\n            if self.args.train_hr:\n                self.dataset_hr_train = VideoMatteDataset(\n                    videomatte_dir=DATA_PATHS['videomatte']['train'],\n                    background_image_dir=DATA_PATHS['background_images']['train'],\n                    size=self.args.resolution_hr,\n                    seq_length=self.args.seq_length_hr,\n                    seq_sampler=TrainFrameSampler(),\n                    transform=VideoMatteTrainAugmentation(size_hr))\n            self.dataset_valid = VideoMatteDataset(\n                videomatte_dir=DATA_PATHS['videomatte']['valid'],\n                background_image_dir=DATA_PATHS['background_images']['valid'],\n                size=self.args.resolution_hr if self.args.train_hr else self.args.resolution_lr,\n                seq_length=self.args.seq_length_hr if self.args.train_hr else self.args.seq_length_lr,\n                seq_sampler=ValidFrameSampler(),\n                transform=VideoMatteValidAugmentation(size_hr if self.args.train_hr else size_lr))\n        \n            \n        # Matting dataloaders:\n        self.datasampler_lr_train = DistributedSampler(\n            dataset=self.dataset_lr_train,\n            rank=self.rank,\n            num_replicas=self.world_size,\n            shuffle=True)\n        self.dataloader_lr_train = DataLoader(\n            dataset=self.dataset_lr_train,\n            batch_size=self.args.batch_size_per_gpu,\n            num_workers=self.args.num_workers,\n            sampler=self.datasampler_lr_train,\n            pin_memory=True)\n        if self.args.train_hr:\n            self.datasampler_hr_train = DistributedSampler(\n                dataset=self.dataset_hr_train,\n                rank=self.rank,\n                num_replicas=self.world_size,\n                shuffle=True)\n            self.dataloader_hr_train = DataLoader(\n                dataset=self.dataset_hr_train,\n                batch_size=self.args.batch_size_per_gpu,\n                num_workers=self.args.num_workers,\n                sampler=self.datasampler_hr_train,\n                pin_memory=True)\n        self.dataloader_valid = DataLoader(\n            dataset=self.dataset_valid,\n            batch_size=self.args.batch_size_per_gpu,\n            num_workers=self.args.num_workers,\n            pin_memory=True)\n        \n        # Segementation datasets\n        '''\n        self.log('Initializing image segmentation datasets')\n        self.dataset_seg_image = ConcatDataset([\n            CocoPanopticDataset(\n                imgdir=DATA_PATHS['coco_panoptic']['imgdir'],\n                anndir=DATA_PATHS['coco_panoptic']['anndir'],\n                annfile=DATA_PATHS['coco_panoptic']['annfile'],\n                transform=CocoPanopticTrainAugmentation(size_lr)),\n            SuperviselyPersonDataset(\n                imgdir=DATA_PATHS['spd']['imgdir'],\n                segdir=DATA_PATHS['spd']['segdir'],\n                transform=CocoPanopticTrainAugmentation(size_lr))\n        ])\n        \n        self.datasampler_seg_image = DistributedSampler(\n            dataset=self.dataset_seg_image,\n            rank=self.rank,\n            num_replicas=self.world_size,\n            shuffle=True)\n        self.dataloader_seg_image = DataLoader(\n            dataset=self.dataset_seg_image,\n            batch_size=self.args.batch_size_per_gpu * self.args.seq_length_lr,\n            num_workers=self.args.num_workers,\n            sampler=self.datasampler_seg_image,\n            pin_memory=True)\n        '''\n        \n        \n    def init_model(self):\n        self.log('Initializing model')\n        self.model = MattingNetwork(self.args.model_variant, pretrained_backbone=True).to(self.rank)\n        \n        if self.args.checkpoint:\n            self.log(f'Restoring from checkpoint: {self.args.checkpoint}')\n            self.log(self.model.load_state_dict(\n                torch.load(self.args.checkpoint, map_location=f'cuda:{self.rank}')))\n            \n        self.model = nn.SyncBatchNorm.convert_sync_batchnorm(self.model)\n        self.model_ddp = DDP(self.model, device_ids=[self.rank], broadcast_buffers=False, find_unused_parameters=True)\n        self.optimizer = Adam([\n            {'params': self.model.backbone.parameters(), 'lr': self.args.learning_rate_backbone},\n            {'params': self.model.aspp.parameters(), 'lr': self.args.learning_rate_aspp},\n            {'params': self.model.decoder.parameters(), 'lr': self.args.learning_rate_decoder},\n            {'params': self.model.project_mat.parameters(), 'lr': self.args.learning_rate_decoder},\n            {'params': self.model.project_seg.parameters(), 'lr': self.args.learning_rate_decoder},\n            {'params': self.model.refiner.parameters(), 'lr': self.args.learning_rate_refiner},\n        ])\n        self.scaler = GradScaler()\n        \n    def init_writer(self):\n        if self.rank == 0:\n            self.log('Initializing writer')\n            self.writer = SummaryWriter(self.args.log_dir)\n        \n    def train(self):\n        for epoch in range(self.args.epoch_start, self.args.epoch_end):\n            self.epoch = epoch\n            self.step = epoch * len(self.dataloader_lr_train)\n            \n            if not self.args.disable_validation:\n                self.validate()\n            \n            self.log(f'Training epoch: {epoch}')\n            for true_fgr, true_pha, true_bgr in tqdm(self.dataloader_lr_train, disable=self.args.disable_progress_bar, dynamic_ncols=True):\n                # Low resolution pass\n                self.train_mat(true_fgr, true_pha, true_bgr, downsample_ratio=1, tag='lr')\n\n                # High resolution pass\n                if self.args.train_hr:\n                    true_fgr, true_pha, true_bgr = self.load_next_mat_hr_sample()\n                    self.train_mat(true_fgr, true_pha, true_bgr, downsample_ratio=self.args.downsample_ratio, tag='hr')\n                '''\n                # Segmentation pass\n                if self.step % 2 == 0:\n                    true_img, true_seg = self.load_next_seg_video_sample()\n                    self.train_seg(true_img, true_seg, log_label='seg_video')\n                else:\n                    true_img, true_seg = self.load_next_seg_image_sample()\n                    self.train_seg(true_img.unsqueeze(1), true_seg.unsqueeze(1), log_label='seg_image')\n                '''\n                if self.step % self.args.checkpoint_save_interval == 0:\n                    self.save()\n                    \n                self.step += 1\n                \n    def train_mat(self, true_fgr, true_pha, true_bgr, downsample_ratio, tag):\n        true_fgr = true_fgr.to(self.rank, non_blocking=True)\n        true_pha = true_pha.to(self.rank, non_blocking=True)\n        true_bgr = true_bgr.to(self.rank, non_blocking=True)\n        true_fgr, true_pha, true_bgr = self.random_crop(true_fgr, true_pha, true_bgr)\n        true_src = true_fgr * true_pha + true_bgr * (1 - true_pha)\n        \n        with autocast(enabled=not self.args.disable_mixed_precision):\n            pred_fgr, pred_pha = self.model_ddp(true_src, downsample_ratio=downsample_ratio)[:2]\n            loss = matting_loss(pred_fgr, pred_pha, true_fgr, true_pha)\n\n        self.scaler.scale(loss['total']).backward()\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n        self.optimizer.zero_grad()\n        \n        if self.rank == 0 and self.step % self.args.log_train_loss_interval == 0:\n            for loss_name, loss_value in loss.items():\n                self.writer.add_scalar(f'train_{tag}_{loss_name}', loss_value, self.step)\n            \n        if self.rank == 0 and self.step % self.args.log_train_images_interval == 0:\n            self.writer.add_image(f'train_{tag}_pred_fgr', make_grid(pred_fgr.flatten(0, 1), nrow=pred_fgr.size(1)), self.step)\n            self.writer.add_image(f'train_{tag}_pred_pha', make_grid(pred_pha.flatten(0, 1), nrow=pred_pha.size(1)), self.step)\n            self.writer.add_image(f'train_{tag}_true_fgr', make_grid(true_fgr.flatten(0, 1), nrow=true_fgr.size(1)), self.step)\n            self.writer.add_image(f'train_{tag}_true_pha', make_grid(true_pha.flatten(0, 1), nrow=true_pha.size(1)), self.step)\n            self.writer.add_image(f'train_{tag}_true_src', make_grid(true_src.flatten(0, 1), nrow=true_src.size(1)), self.step)\n            \n    def train_seg(self, true_img, true_seg, log_label):\n        true_img = true_img.to(self.rank, non_blocking=True)\n        true_seg = true_seg.to(self.rank, non_blocking=True)\n        \n        true_img, true_seg = self.random_crop(true_img, true_seg)\n        \n        with autocast(enabled=not self.args.disable_mixed_precision):\n            pred_seg = self.model_ddp(true_img, segmentation_pass=True)[0]\n            loss = segmentation_loss(pred_seg, true_seg)\n        \n        self.scaler.scale(loss).backward()\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n        self.optimizer.zero_grad()\n        \n        if self.rank == 0 and (self.step - self.step % 2) % self.args.log_train_loss_interval == 0:\n            self.writer.add_scalar(f'{log_label}_loss', loss, self.step)\n        \n        if self.rank == 0 and (self.step - self.step % 2) % self.args.log_train_images_interval == 0:\n            self.writer.add_image(f'{log_label}_pred_seg', make_grid(pred_seg.flatten(0, 1).float().sigmoid(), nrow=self.args.seq_length_lr), self.step)\n            self.writer.add_image(f'{log_label}_true_seg', make_grid(true_seg.flatten(0, 1), nrow=self.args.seq_length_lr), self.step)\n            self.writer.add_image(f'{log_label}_true_img', make_grid(true_img.flatten(0, 1), nrow=self.args.seq_length_lr), self.step)\n    \n    def load_next_mat_hr_sample(self):\n        try:\n            sample = next(self.dataiterator_mat_hr)\n        except:\n            self.datasampler_hr_train.set_epoch(self.datasampler_hr_train.epoch + 1)\n            self.dataiterator_mat_hr = iter(self.dataloader_hr_train)\n            sample = next(self.dataiterator_mat_hr)\n        return sample\n    \n    def load_next_seg_video_sample(self):\n        try:\n            sample = next(self.dataiterator_seg_video)\n        except:\n            self.datasampler_seg_video.set_epoch(self.datasampler_seg_video.epoch + 1)\n            self.dataiterator_seg_video = iter(self.dataloader_seg_video)\n            sample = next(self.dataiterator_seg_video)\n        return sample\n    \n    def load_next_seg_image_sample(self):\n        try:\n            sample = next(self.dataiterator_seg_image)\n        except:\n            self.datasampler_seg_image.set_epoch(self.datasampler_seg_image.epoch + 1)\n            self.dataiterator_seg_image = iter(self.dataloader_seg_image)\n            sample = next(self.dataiterator_seg_image)\n        return sample\n    \n    def validate(self):\n        if self.rank == 0:\n            self.log(f'Validating at the start of epoch: {self.epoch}')\n            self.model_ddp.eval()\n            total_loss, total_count = 0, 0\n            with torch.no_grad():\n                with autocast(enabled=not self.args.disable_mixed_precision):\n                    for true_fgr, true_pha, true_bgr in tqdm(self.dataloader_valid, disable=self.args.disable_progress_bar, dynamic_ncols=True):\n                        true_fgr = true_fgr.to(self.rank, non_blocking=True)\n                        true_pha = true_pha.to(self.rank, non_blocking=True)\n                        true_bgr = true_bgr.to(self.rank, non_blocking=True)\n                        true_src = true_fgr * true_pha + true_bgr * (1 - true_pha)\n                        batch_size = true_src.size(0)\n                        pred_fgr, pred_pha = self.model(true_src)[:2]\n                        total_loss += matting_loss(pred_fgr, pred_pha, true_fgr, true_pha)['total'].item() * batch_size\n                        total_count += batch_size\n            avg_loss = total_loss / total_count\n            self.log(f'Validation set average loss: {avg_loss}')\n            self.writer.add_scalar('valid_loss', avg_loss, self.step)\n            self.model_ddp.train()\n        dist.barrier()\n    \n    def random_crop(self, *imgs):\n        h, w = imgs[0].shape[-2:]\n        w = random.choice(range(w // 2, w))\n        h = random.choice(range(h // 2, h))\n        results = []\n        for img in imgs:\n            B, T = img.shape[:2]\n            img = img.flatten(0, 1)\n            img = F.interpolate(img, (max(h, w), max(h, w)), mode='bilinear', align_corners=False)\n            img = center_crop(img, (h, w))\n            img = img.reshape(B, T, *img.shape[1:])\n            results.append(img)\n        return results\n    \n    def save(self):\n        if self.rank == 0:\n            os.makedirs(self.args.checkpoint_dir, exist_ok=True)\n            torch.save(self.model.state_dict(), os.path.join(self.args.checkpoint_dir, f'epoch-{self.epoch}.pth'))\n            self.log('Model saved')\n        dist.barrier()\n        \n    def cleanup(self):\n        dist.destroy_process_group()\n        \n    def log(self, msg):\n        print(f'[GPU{self.rank}] {msg}')\n\nif __name__ == '__main__':\n    world_size = torch.cuda.device_count()\n    mp.spawn(\n        Trainer,\n        nprocs=world_size,\n        args=(world_size,),\n        join=True)\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:12:39.373128Z","iopub.execute_input":"2025-06-14T18:12:39.373435Z","iopub.status.idle":"2025-06-14T18:12:39.401159Z","shell.execute_reply.started":"2025-06-14T18:12:39.373413Z","shell.execute_reply":"2025-06-14T18:12:39.400181Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"!rm -rf main.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:12:41.645149Z","iopub.execute_input":"2025-06-14T18:12:41.645734Z","iopub.status.idle":"2025-06-14T18:12:41.811623Z","shell.execute_reply.started":"2025-06-14T18:12:41.645711Z","shell.execute_reply":"2025-06-14T18:12:41.810594Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"with open(\"main.py\", \"w\") as f:\n    f.write(code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:12:43.724627Z","iopub.execute_input":"2025-06-14T18:12:43.724951Z","iopub.status.idle":"2025-06-14T18:12:43.729542Z","shell.execute_reply.started":"2025-06-14T18:12:43.724924Z","shell.execute_reply":"2025-06-14T18:12:43.728875Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"!python main.py \\\n    --model-variant mobilenetv3 \\\n    --dataset videomatte \\\n    --resolution-lr 512 \\\n    --seq-length-lr 15 \\\n    --learning-rate-backbone 0.0001 \\\n    --learning-rate-aspp 0.0002 \\\n    --learning-rate-decoder 0.0002 \\\n    --learning-rate-refiner 0 \\\n    --checkpoint-dir checkpoint/stage1 \\\n    --log-dir log/stage1 \\\n    --epoch-start 0 \\\n    --epoch-end 5","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-14T18:12:46.740132Z","iopub.execute_input":"2025-06-14T18:12:46.740447Z","iopub.status.idle":"2025-06-14T21:05:06.034172Z","shell.execute_reply.started":"2025-06-14T18:12:46.740427Z","shell.execute_reply":"2025-06-14T21:05:06.031463Z"}},"outputs":[{"name":"stdout","text":"2025-06-14 18:12:50.193037: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924770.212931      93 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924770.219060      93 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:12:58.905634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924778.932257     106 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924778.943167     106 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[GPU0] Initializing distributed\n[GPU0] Initializing matting datasets\n/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n[GPU0] Initializing model\nDownloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v3_large-8738ca79.pth\n100%|███████████████████████████████████████| 21.1M/21.1M [00:00<00:00, 184MB/s]\n/kaggle/working/main.py:1289: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = GradScaler()\n[GPU0] Initializing writer\n[GPU0] Validating at the start of epoch: 0\n/kaggle/working/main.py:1410: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=not self.args.disable_mixed_precision):\n  0%|                                                   | 0/184 [00:00<?, ?it/s]2025-06-14 18:13:13.837673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924793.914701     135 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924793.934812     135 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:13:13.979316: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924794.063726     133 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924794.088740     133 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:13:14.435068: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924794.507961     134 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924794.531167     134 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:13:15.018598: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-14 18:13:15.062638: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924795.120763     131 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924795.136925     130 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924795.140950     131 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1749924795.157263     130 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:13:15.339484: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924795.426360     136 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924795.437402     136 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:13:15.592342: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924795.668033     132 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924795.694049     132 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:13:15.811343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924795.887953     137 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924795.913165     137 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n100%|█████████████████████████████████████████| 184/184 [02:02<00:00,  1.50it/s]\n[GPU0] Validation set average loss: 0.42234512260588614\n[GPU0] Training epoch: 0\n  0%|                                                 | 0/16088 [00:00<?, ?it/s]2025-06-14 18:15:10.663653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924910.684895     244 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924910.691500     244 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:15:18.946301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924918.966990     255 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924918.973574     255 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:15:27.038455: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924927.058465     266 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924927.065180     266 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:15:35.138306: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924935.158241     277 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924935.165356     277 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:15:43.284609: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924943.305854     288 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924943.312402     288 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:15:51.383960: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924951.403854     299 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924951.410287     299 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:15:59.490722: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924959.511533     310 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924959.518035     310 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n2025-06-14 18:16:07.432101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1749924967.453323     321 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1749924967.459583     321 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/kaggle/working/main.py:1334: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast(enabled=not self.args.disable_mixed_precision):\n[GPU0] Model saved\n  3%|█▏                                   | 500/16088 [08:31<3:36:03,  1.20it/s][GPU0] Model saved\n  6%|██▏                                 | 1000/16088 [15:41<3:17:50,  1.27it/s][GPU0] Model saved\n  9%|███▎                                | 1500/16088 [22:45<3:31:33,  1.15it/s][GPU0] Model saved\n 12%|████▍                               | 2000/16088 [30:09<3:22:04,  1.16it/s][GPU0] Model saved\n 16%|█████▌                              | 2500/16088 [37:22<2:43:07,  1.39it/s][GPU0] Model saved\n 19%|██████▋                             | 3000/16088 [44:42<3:29:53,  1.04it/s][GPU0] Model saved\n 22%|███████▊                            | 3500/16088 [51:48<2:40:54,  1.30it/s][GPU0] Model saved\n 25%|████████▉                           | 4000/16088 [59:08<3:16:13,  1.03it/s][GPU0] Model saved\n 28%|█████████▌                        | 4500/16088 [1:06:25<2:44:38,  1.17it/s][GPU0] Model saved\n 31%|██████████▌                       | 5000/16088 [1:13:48<2:52:01,  1.07it/s][GPU0] Model saved\n 34%|███████████▌                      | 5500/16088 [1:21:10<2:12:59,  1.33it/s][GPU0] Model saved\n 37%|████████████▋                     | 6000/16088 [1:28:28<2:07:21,  1.32it/s][GPU0] Model saved\n 40%|█████████████▋                    | 6500/16088 [1:35:30<2:00:19,  1.33it/s][GPU0] Model saved\n 44%|██████████████▊                   | 7000/16088 [1:42:33<2:17:24,  1.10it/s][GPU0] Model saved\n 47%|███████████████▊                  | 7500/16088 [1:49:37<1:59:36,  1.20it/s][GPU0] Model saved\n 50%|████████████████▉                 | 8000/16088 [1:56:42<1:34:48,  1.42it/s][GPU0] Model saved\n 53%|█████████████████▉                | 8500/16088 [2:03:35<1:38:26,  1.28it/s][GPU0] Model saved\n 56%|███████████████████               | 9000/16088 [2:10:38<1:49:49,  1.08it/s][GPU0] Model saved\n 59%|████████████████████              | 9500/16088 [2:17:32<2:52:55,  1.57s/it][GPU0] Model saved\n 62%|████████████████████▌            | 10000/16088 [2:24:33<1:20:06,  1.27it/s][GPU0] Model saved\n 65%|█████████████████████▌           | 10500/16088 [2:31:36<1:06:28,  1.40it/s][GPU0] Model saved\n 68%|██████████████████████▌          | 11000/16088 [2:38:29<1:11:39,  1.18it/s][GPU0] Model saved\n 71%|███████████████████████▌         | 11500/16088 [2:45:17<1:01:59,  1.23it/s][GPU0] Model saved\n 74%|█████████████████████████▊         | 11839/16088 [2:49:57<53:04,  1.33it/s]^C\nTraceback (most recent call last):\n  File \"/kaggle/working/main.py\", line 1455, in <module>\n    mp.spawn(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 340, in spawn\n    return start_processes(fn, args, nprocs, join, daemon, start_method=\"spawn\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 296, in start_processes\n    while not context.join():\n              ^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/spawn.py\", line 144, in join\n    ready = multiprocessing.connection.wait(\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/multiprocessing/connection.py\", line 948, in wait\n    ready = selector.select(timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.11/selectors.py\", line 415, in select\n    fd_event_list = self._selector.poll(timeout)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nKeyboardInterrupt\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"Конца обучения не дождался, но 23 эпохи было обучено, а словарь с весами сохранён","metadata":{}}]}