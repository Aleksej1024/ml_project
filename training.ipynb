{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6e98f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import shutil\n",
    "import tarfile\n",
    "import argparse\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "from torch import nn\n",
    "from torch import distributed as dist\n",
    "from torch import multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.optim import Adam\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.functional import center_crop\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as FT\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from typing import Tuple, Optional, List\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.mobilenetv3 import MobileNetV3, InvertedResidualConfig\n",
    "from torchvision.transforms.functional import normalize\n",
    "import easing_functions as ef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "657e1221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1-S4F-rB75E8I7YUpHfu3itIl1knFhhFF\n",
      "From (redirected): https://drive.google.com/uc?id=1-S4F-rB75E8I7YUpHfu3itIl1knFhhFF&confirm=t&uuid=f62f7c43-9652-413d-9354-4d3bf56824a7\n",
      "To: /home/aleksej/Документы/Универ/6 семак/Samsung/rvm_aleksej/VideoMatte240K_JPEG_SD.tar\n",
      "100%|██████████| 6.11G/6.11G [20:58<00:00, 4.86MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./VideoMatte240K_JPEG_SD.tar'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://drive.google.com/uc?id=1-S4F-rB75E8I7YUpHfu3itIl1knFhhFF'\n",
    "output = './VideoMatte240K_JPEG_SD.tar'\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ccd6191e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FqD-HfwXwbeTswQEIFaQkaVWUh_i6cSy\n",
      "To: /home/aleksej/Документы/Универ/6 семак/Samsung/rvm_aleksej/Backgrounds_Validation.tar\n",
      "100%|██████████| 57.4M/57.4M [00:08<00:00, 6.91MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./Backgrounds_Validation.tar'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://drive.google.com/uc?id=1FqD-HfwXwbeTswQEIFaQkaVWUh_i6cSy'\n",
    "output = './Backgrounds_Validation.tar'\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16c9ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/.private/aleksej/ipykernel_89755/1971101141.py:2: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  zip_file.extractall('./')\n"
     ]
    }
   ],
   "source": [
    "with tarfile.open('./VideoMatte240K_JPEG_SD.tar', 'r') as zip_file:\n",
    "    zip_file.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a20f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/.private/aleksej/ipykernel_89755/2972307017.py:5: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tar_file.extractall('./ImageMatte')\n"
     ]
    }
   ],
   "source": [
    "train_path = './Backgrounds/train'\n",
    "valid_path = './Backgrounds/valid'\n",
    "# Извлечение файлов из архива\n",
    "with tarfile.open('./Backgrounds_Validation.tar', 'r') as tar_file:\n",
    "    tar_file.extractall('./ImageMatte')\n",
    "# Получение списка файлов\n",
    "files = os.listdir('./ImageMatte/Backgrounds')\n",
    "# Перемешивание файлов\n",
    "random.shuffle(files)\n",
    "# Определение количества файлов для обучения и валидации\n",
    "train_count = int(len(files) * 0.8)\n",
    "train_files = files[:train_count]\n",
    "valid_files = files[train_count:]\n",
    "# Создание папок, если они не существуют\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(valid_path, exist_ok=True)\n",
    "# Перемещение файлов в соответствующие папки\n",
    "for file in train_files:\n",
    "    shutil.move(os.path.join('./ImageMatte/Backgrounds', file), os.path.join(train_path, file))\n",
    "for file in valid_files:\n",
    "    shutil.move(os.path.join('./ImageMatte/Backgrounds', file), os.path.join(valid_path, file))\n",
    "# Удаление временной папки с извлеченными файлами\n",
    "shutil.rmtree('./ImageMatte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96ee636",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATHS = {\n",
    "\n",
    "    'videomatte': {\n",
    "        'train': './VideoMatte240K_JPEG_SD/train',\n",
    "        'valid': './VideoMatte240K_JPEG_SD/valid',\n",
    "    },\n",
    "    'background_images': {\n",
    "            'train': './Backgrounds/train',\n",
    "            'valid': './Backgrounds/valid',\n",
    "    },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee76f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10aacd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoMatteDataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 videomatte_dir,\n",
    "                 background_image_dir,\n",
    "                 size,\n",
    "                 seq_length,\n",
    "                 seq_sampler,\n",
    "                 transform=None):\n",
    "        self.background_image_dir = background_image_dir\n",
    "        self.background_image_files = os.listdir(background_image_dir)\n",
    "        \n",
    "        self.videomatte_dir = videomatte_dir\n",
    "        self.videomatte_clips = sorted(os.listdir(os.path.join(videomatte_dir, 'fgr')))\n",
    "        self.videomatte_frames = [sorted(os.listdir(os.path.join(videomatte_dir, 'fgr', clip))) \n",
    "                                  for clip in self.videomatte_clips]\n",
    "        self.videomatte_idx = [(clip_idx, frame_idx) \n",
    "                               for clip_idx in range(len(self.videomatte_clips)) \n",
    "                               for frame_idx in range(0, len(self.videomatte_frames[clip_idx]), seq_length)]\n",
    "        self.size = size\n",
    "        self.seq_length = seq_length\n",
    "        self.seq_sampler = seq_sampler\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videomatte_idx)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        bgrs = self._get_random_image_background()\n",
    "\n",
    "        \n",
    "        fgrs, phas = self._get_videomatte(idx)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            return self.transform(fgrs, phas, bgrs)\n",
    "        \n",
    "        return fgrs, phas, bgrs\n",
    "    \n",
    "    def _get_random_image_background(self):\n",
    "        with Image.open(os.path.join(self.background_image_dir, random.choice(self.background_image_files))) as bgr:\n",
    "            bgr = self._downsample_if_needed(bgr.convert('RGB'))\n",
    "        bgrs = [bgr] * self.seq_length\n",
    "        return bgrs\n",
    "    \n",
    "    \n",
    "    def _get_videomatte(self, idx):\n",
    "        clip_idx, frame_idx = self.videomatte_idx[idx]\n",
    "        clip = self.videomatte_clips[clip_idx]\n",
    "        frame_count = len(self.videomatte_frames[clip_idx])\n",
    "        fgrs, phas = [], []\n",
    "        for i in self.seq_sampler(self.seq_length):\n",
    "            frame = self.videomatte_frames[clip_idx][(frame_idx + i) % frame_count]\n",
    "            with Image.open(os.path.join(self.videomatte_dir, 'fgr', clip, frame)) as fgr, \\\n",
    "                 Image.open(os.path.join(self.videomatte_dir, 'pha', clip, frame)) as pha:\n",
    "                    fgr = self._downsample_if_needed(fgr.convert('RGB'))\n",
    "                    pha = self._downsample_if_needed(pha.convert('L'))\n",
    "            fgrs.append(fgr)\n",
    "            phas.append(pha)\n",
    "        return fgrs, phas\n",
    "    \n",
    "    def _downsample_if_needed(self, img):\n",
    "        w, h = img.size\n",
    "        if min(w, h) > self.size:\n",
    "            scale = self.size / min(w, h)\n",
    "            w = int(scale * w)\n",
    "            h = int(scale * h)\n",
    "            img = img.resize((w, h))\n",
    "        return img\n",
    "\n",
    "class VideoMatteTrainAugmentation(MotionAugmentation):\n",
    "    def __init__(self, size):\n",
    "        super().__init__(\n",
    "            size=size,\n",
    "            prob_fgr_affine=0.3,\n",
    "            prob_bgr_affine=0.3,\n",
    "            prob_noise=0.1,\n",
    "            prob_color_jitter=0.3,\n",
    "            prob_grayscale=0.02,\n",
    "            prob_sharpness=0.1,\n",
    "            prob_blur=0.02,\n",
    "            prob_hflip=0.5,\n",
    "            prob_pause=0.03,\n",
    "        )\n",
    "\n",
    "class VideoMatteValidAugmentation(MotionAugmentation):\n",
    "    def __init__(self, size):\n",
    "        super().__init__(\n",
    "            size=size,\n",
    "            prob_fgr_affine=0,\n",
    "            prob_bgr_affine=0,\n",
    "            prob_noise=0,\n",
    "            prob_color_jitter=0,\n",
    "            prob_grayscale=0,\n",
    "            prob_sharpness=0,\n",
    "            prob_blur=0,\n",
    "            prob_hflip=0,\n",
    "            prob_pause=0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8904eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionAugmentation:\n",
    "    def __init__(self,\n",
    "                 size,\n",
    "                 prob_fgr_affine,\n",
    "                 prob_bgr_affine,\n",
    "                 prob_noise,\n",
    "                 prob_color_jitter,\n",
    "                 prob_grayscale,\n",
    "                 prob_sharpness,\n",
    "                 prob_blur,\n",
    "                 prob_hflip,\n",
    "                 prob_pause,\n",
    "                 static_affine=True,\n",
    "                 aspect_ratio_range=(0.9, 1.1)):\n",
    "        self.size = size\n",
    "        self.prob_fgr_affine = prob_fgr_affine\n",
    "        self.prob_bgr_affine = prob_bgr_affine\n",
    "        self.prob_noise = prob_noise\n",
    "        self.prob_color_jitter = prob_color_jitter\n",
    "        self.prob_grayscale = prob_grayscale\n",
    "        self.prob_sharpness = prob_sharpness\n",
    "        self.prob_blur = prob_blur\n",
    "        self.prob_hflip = prob_hflip\n",
    "        self.prob_pause = prob_pause\n",
    "        self.static_affine = static_affine\n",
    "        self.aspect_ratio_range = aspect_ratio_range\n",
    "        \n",
    "    def __call__(self, fgrs, phas, bgrs):\n",
    "        # Foreground affine\n",
    "        if random.random() < self.prob_fgr_affine:\n",
    "            fgrs, phas = self._motion_affine(fgrs, phas)\n",
    "\n",
    "        # Background affine\n",
    "        if random.random() < self.prob_bgr_affine / 2:\n",
    "            bgrs = self._motion_affine(bgrs)\n",
    "        if random.random() < self.prob_bgr_affine / 2:\n",
    "            fgrs, phas, bgrs = self._motion_affine(fgrs, phas, bgrs)\n",
    "                \n",
    "        # Still Affine\n",
    "        if self.static_affine:\n",
    "            fgrs, phas = self._static_affine(fgrs, phas, scale_ranges=(0.5, 1))\n",
    "            bgrs = self._static_affine(bgrs, scale_ranges=(1, 1.5))\n",
    "        \n",
    "        # To tensor\n",
    "        fgrs = torch.stack([FT.to_tensor(fgr) for fgr in fgrs])\n",
    "        phas = torch.stack([FT.to_tensor(pha) for pha in phas])\n",
    "        bgrs = torch.stack([FT.to_tensor(bgr) for bgr in bgrs])\n",
    "        \n",
    "        # Resize\n",
    "        params = transforms.RandomResizedCrop.get_params(fgrs, scale=(1, 1), ratio=self.aspect_ratio_range)\n",
    "        fgrs = FT.resized_crop(fgrs, *params, self.size, interpolation=FT.InterpolationMode.BILINEAR)\n",
    "        phas = FT.resized_crop(phas, *params, self.size, interpolation=FT.InterpolationMode.BILINEAR)\n",
    "        params = transforms.RandomResizedCrop.get_params(bgrs, scale=(1, 1), ratio=self.aspect_ratio_range)\n",
    "        bgrs = FT.resized_crop(bgrs, *params, self.size, interpolation=FT.InterpolationMode.BILINEAR)\n",
    "\n",
    "        # Horizontal flip\n",
    "        if random.random() < self.prob_hflip:\n",
    "            fgrs = FT.hflip(fgrs)\n",
    "            phas = FT.hflip(phas)\n",
    "        if random.random() < self.prob_hflip:\n",
    "            bgrs = FT.hflip(bgrs)\n",
    "\n",
    "        # Noise\n",
    "        if random.random() < self.prob_noise:\n",
    "            fgrs, bgrs = self._motion_noise(fgrs, bgrs)\n",
    "        \n",
    "        # Color jitter\n",
    "        if random.random() < self.prob_color_jitter:\n",
    "            fgrs = self._motion_color_jitter(fgrs)\n",
    "        if random.random() < self.prob_color_jitter:\n",
    "            bgrs = self._motion_color_jitter(bgrs)\n",
    "            \n",
    "        # Grayscale\n",
    "        if random.random() < self.prob_grayscale:\n",
    "            fgrs = FT.rgb_to_grayscale(fgrs, num_output_channels=3).contiguous()\n",
    "            bgrs = FT.rgb_to_grayscale(bgrs, num_output_channels=3).contiguous()\n",
    "            \n",
    "        # Sharpen\n",
    "        if random.random() < self.prob_sharpness:\n",
    "            sharpness = random.random() * 8\n",
    "            fgrs = FT.adjust_sharpness(fgrs, sharpness)\n",
    "            phas = FT.adjust_sharpness(phas, sharpness)\n",
    "            bgrs = FT.adjust_sharpness(bgrs, sharpness)\n",
    "        \n",
    "        # Blur\n",
    "        if random.random() < self.prob_blur / 3:\n",
    "            fgrs, phas = self._motion_blur(fgrs, phas)\n",
    "        if random.random() < self.prob_blur / 3:\n",
    "            bgrs = self._motion_blur(bgrs)\n",
    "        if random.random() < self.prob_blur / 3:\n",
    "            fgrs, phas, bgrs = self._motion_blur(fgrs, phas, bgrs)\n",
    "\n",
    "        # Pause\n",
    "        if random.random() < self.prob_pause:\n",
    "            fgrs, phas, bgrs = self._motion_pause(fgrs, phas, bgrs)\n",
    "        \n",
    "        return fgrs, phas, bgrs\n",
    "    \n",
    "    def _static_affine(self, *imgs, scale_ranges):\n",
    "        params = transforms.RandomAffine.get_params(\n",
    "            degrees=(-10, 10), translate=(0.1, 0.1), scale_ranges=scale_ranges,\n",
    "            shears=(-5, 5), img_size=imgs[0][0].size)\n",
    "        imgs = [[FT.affine(t, *params, FT.InterpolationMode.BILINEAR) for t in img] for img in imgs]\n",
    "        return imgs if len(imgs) > 1 else imgs[0] \n",
    "    \n",
    "    def _motion_affine(self, *imgs):\n",
    "        config = dict(degrees=(-10, 10), translate=(0.1, 0.1),\n",
    "                      scale_ranges=(0.9, 1.1), shears=(-5, 5), img_size=imgs[0][0].size)\n",
    "        angleA, (transXA, transYA), scaleA, (shearXA, shearYA) = transforms.RandomAffine.get_params(**config)\n",
    "        angleB, (transXB, transYB), scaleB, (shearXB, shearYB) = transforms.RandomAffine.get_params(**config)\n",
    "        \n",
    "        T = len(imgs[0])\n",
    "        easing = random_easing_fn()\n",
    "        for t in range(T):\n",
    "            percentage = easing(t / (T - 1))\n",
    "            angle = lerp(angleA, angleB, percentage)\n",
    "            transX = lerp(transXA, transXB, percentage)\n",
    "            transY = lerp(transYA, transYB, percentage)\n",
    "            scale = lerp(scaleA, scaleB, percentage)\n",
    "            shearX = lerp(shearXA, shearXB, percentage)\n",
    "            shearY = lerp(shearYA, shearYB, percentage)\n",
    "            for img in imgs:\n",
    "                img[t] = FT.affine(img[t], angle, (transX, transY), scale, (shearX, shearY), FT.InterpolationMode.BILINEAR)\n",
    "        return imgs if len(imgs) > 1 else imgs[0]\n",
    "    \n",
    "    def _motion_noise(self, *imgs):\n",
    "        grain_size = random.random() * 3 + 1 # range 1 ~ 4\n",
    "        monochrome = random.random() < 0.5\n",
    "        for img in imgs:\n",
    "            T, C, H, W = img.shape\n",
    "            noise = torch.randn((T, 1 if monochrome else C, round(H / grain_size), round(W / grain_size)))\n",
    "            noise.mul_(random.random() * 0.2 / grain_size)\n",
    "            if grain_size != 1:\n",
    "                noise = FT.resize(noise, (H, W))\n",
    "            img.add_(noise).clamp_(0, 1)\n",
    "        return imgs if len(imgs) > 1 else imgs[0]\n",
    "    \n",
    "    def _motion_color_jitter(self, *imgs):\n",
    "        brightnessA, brightnessB, contrastA, contrastB, saturationA, saturationB, hueA, hueB \\\n",
    "            = torch.randn(8).mul(0.1).tolist()\n",
    "        strength = random.random() * 0.2\n",
    "        easing = random_easing_fn()\n",
    "        T = len(imgs[0])\n",
    "        for t in range(T):\n",
    "            percentage = easing(t / (T - 1)) * strength\n",
    "            for img in imgs:\n",
    "                img[t] = FT.adjust_brightness(img[t], max(1 + lerp(brightnessA, brightnessB, percentage), 0.1))\n",
    "                img[t] = FT.adjust_contrast(img[t], max(1 + lerp(contrastA, contrastB, percentage), 0.1))\n",
    "                img[t] = FT.adjust_saturation(img[t], max(1 + lerp(brightnessA, brightnessB, percentage), 0.1))\n",
    "                img[t] = FT.adjust_hue(img[t], min(0.5, max(-0.5, lerp(hueA, hueB, percentage) * 0.1)))\n",
    "        return imgs if len(imgs) > 1 else imgs[0]\n",
    "    \n",
    "    def _motion_blur(self, *imgs):\n",
    "        blurA = random.random() * 10\n",
    "        blurB = random.random() * 10\n",
    "\n",
    "        T = len(imgs[0])\n",
    "        easing = random_easing_fn()\n",
    "        for t in range(T):\n",
    "            percentage = easing(t / (T - 1))\n",
    "            blur = max(lerp(blurA, blurB, percentage), 0)\n",
    "            if blur != 0:\n",
    "                kernel_size = int(blur * 2)\n",
    "                if kernel_size % 2 == 0:\n",
    "                    kernel_size += 1 # Make kernel_size odd\n",
    "                for img in imgs:\n",
    "                    img[t] = FT.gaussian_blur(img[t], kernel_size, sigma=blur)\n",
    "    \n",
    "        return imgs if len(imgs) > 1 else imgs[0]\n",
    "    \n",
    "    def _motion_pause(self, *imgs):\n",
    "        T = len(imgs[0])\n",
    "        pause_frame = random.choice(range(T - 1))\n",
    "        pause_length = random.choice(range(T - pause_frame))\n",
    "        for img in imgs:\n",
    "            img[pause_frame + 1 : pause_frame + pause_length] = img[pause_frame]\n",
    "        return imgs if len(imgs) > 1 else imgs[0]\n",
    "    \n",
    "\n",
    "def lerp(a, b, percentage):\n",
    "    return a * (1 - percentage) + b * percentage\n",
    "\n",
    "\n",
    "def random_easing_fn():\n",
    "    if random.random() < 0.2:\n",
    "        return ef.LinearInOut()\n",
    "    else:\n",
    "        return random.choice([\n",
    "            ef.BackEaseIn,\n",
    "            ef.BackEaseOut,\n",
    "            ef.BackEaseInOut,\n",
    "            ef.BounceEaseIn,\n",
    "            ef.BounceEaseOut,\n",
    "            ef.BounceEaseInOut,\n",
    "            ef.CircularEaseIn,\n",
    "            ef.CircularEaseOut,\n",
    "            ef.CircularEaseInOut,\n",
    "            ef.CubicEaseIn,\n",
    "            ef.CubicEaseOut,\n",
    "            ef.CubicEaseInOut,\n",
    "            ef.ExponentialEaseIn,\n",
    "            ef.ExponentialEaseOut,\n",
    "            ef.ExponentialEaseInOut,\n",
    "            ef.ElasticEaseIn,\n",
    "            ef.ElasticEaseOut,\n",
    "            ef.ElasticEaseInOut,\n",
    "            ef.QuadEaseIn,\n",
    "            ef.QuadEaseOut,\n",
    "            ef.QuadEaseInOut,\n",
    "            ef.QuarticEaseIn,\n",
    "            ef.QuarticEaseOut,\n",
    "            ef.QuarticEaseInOut,\n",
    "            ef.QuinticEaseIn,\n",
    "            ef.QuinticEaseOut,\n",
    "            ef.QuinticEaseInOut,\n",
    "            ef.SineEaseIn,\n",
    "            ef.SineEaseOut,\n",
    "            ef.SineEaseInOut,\n",
    "            Step,\n",
    "        ])()\n",
    "\n",
    "class Step: # Custom easing function for sudden change.\n",
    "    def __call__(self, value):\n",
    "        return 0 if value < 0.5 else 1\n",
    "\n",
    "\n",
    "# ---------------------------- Frame Sampler ----------------------------\n",
    "\n",
    "\n",
    "class TrainFrameSampler:\n",
    "    def __init__(self, speed=[0.5, 1, 2, 3, 4, 5]):\n",
    "        self.speed = speed\n",
    "    \n",
    "    def __call__(self, seq_length):\n",
    "        frames = list(range(seq_length))\n",
    "        \n",
    "        # Speed up\n",
    "        speed = random.choice(self.speed)\n",
    "        frames = [int(FT * speed) for FT in frames]\n",
    "        \n",
    "        # Shift\n",
    "        shift = random.choice(range(seq_length))\n",
    "        frames = [FT + shift for FT in frames]\n",
    "        \n",
    "        # Reverse\n",
    "        if random.random() < 0.5:\n",
    "            frames = frames[::-1]\n",
    "\n",
    "        return frames\n",
    "    \n",
    "class ValidFrameSampler:\n",
    "    def __call__(self, seq_length):\n",
    "        return range(seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8520e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adopted from <https://github.com/wuhuikai/DeepGuidedFilter/>\n",
    "\"\"\"\n",
    "\n",
    "class FastGuidedFilterRefiner(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        self.guilded_filter = FastGuidedFilter(1)\n",
    "    \n",
    "    def forward_single_frame(self, fine_src, base_src, base_fgr, base_pha):\n",
    "        fine_src_gray = fine_src.mean(1, keepdim=True)\n",
    "        base_src_gray = base_src.mean(1, keepdim=True)\n",
    "        \n",
    "        fgr, pha = self.guilded_filter(\n",
    "            torch.cat([base_src, base_src_gray], dim=1),\n",
    "            torch.cat([base_fgr, base_pha], dim=1),\n",
    "            torch.cat([fine_src, fine_src_gray], dim=1)).split([3, 1], dim=1)\n",
    "        \n",
    "        return fgr, pha\n",
    "    \n",
    "    def forward_time_series(self, fine_src, base_src, base_fgr, base_pha):\n",
    "        B, T = fine_src.shape[:2]\n",
    "        fgr, pha = self.forward_single_frame(\n",
    "            fine_src.flatten(0, 1),\n",
    "            base_src.flatten(0, 1),\n",
    "            base_fgr.flatten(0, 1),\n",
    "            base_pha.flatten(0, 1))\n",
    "        fgr = fgr.unflatten(0, (B, T))\n",
    "        pha = pha.unflatten(0, (B, T))\n",
    "        return fgr, pha\n",
    "    \n",
    "    def forward(self, fine_src, base_src, base_fgr, base_pha, base_hid):\n",
    "        if fine_src.ndim == 5:\n",
    "            return self.forward_time_series(fine_src, base_src, base_fgr, base_pha)\n",
    "        else:\n",
    "            return self.forward_single_frame(fine_src, base_src, base_fgr, base_pha)\n",
    "\n",
    "\n",
    "class FastGuidedFilter(nn.Module):\n",
    "    def __init__(self, r: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.r = r\n",
    "        self.eps = eps\n",
    "        self.boxfilter = BoxFilter(r)\n",
    "\n",
    "    def forward(self, lr_x, lr_y, hr_x):\n",
    "        mean_x = self.boxfilter(lr_x)\n",
    "        mean_y = self.boxfilter(lr_y)\n",
    "        cov_xy = self.boxfilter(lr_x * lr_y) - mean_x * mean_y\n",
    "        var_x = self.boxfilter(lr_x * lr_x) - mean_x * mean_x\n",
    "        A = cov_xy / (var_x + self.eps)\n",
    "        b = mean_y - A * mean_x\n",
    "        A = F.interpolate(A, hr_x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        b = F.interpolate(b, hr_x.shape[2:], mode='bilinear', align_corners=False)\n",
    "        return A * hr_x + b\n",
    "\n",
    "\n",
    "class BoxFilter(nn.Module):\n",
    "    def __init__(self, r):\n",
    "        super(BoxFilter, self).__init__()\n",
    "        self.r = r\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Note: The original implementation at <https://github.com/wuhuikai/DeepGuidedFilter/>\n",
    "        #       uses faster box blur. However, it may not be friendly for ONNX export.\n",
    "        #       We are switching to use simple convolution for box blur.\n",
    "        kernel_size = 2 * self.r + 1\n",
    "        kernel_x = torch.full((x.data.shape[1], 1, 1, kernel_size), 1 / kernel_size, device=x.device, dtype=x.dtype)\n",
    "        kernel_y = torch.full((x.data.shape[1], 1, kernel_size, 1), 1 / kernel_size, device=x.device, dtype=x.dtype)\n",
    "        x = F.conv2d(x, kernel_x, padding=(0, self.r), groups=x.data.shape[1])\n",
    "        x = F.conv2d(x, kernel_y, padding=(self.r, 0), groups=x.data.shape[1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28ce6bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentDecoder(nn.Module):\n",
    "    def __init__(self, feature_channels, decoder_channels):\n",
    "        super().__init__()\n",
    "        self.avgpool = AvgPool()\n",
    "        self.decode4 = BottleneckBlock(feature_channels[3])\n",
    "        self.decode3 = UpsamplingBlock(feature_channels[3], feature_channels[2], 3, decoder_channels[0])\n",
    "        self.decode2 = UpsamplingBlock(decoder_channels[0], feature_channels[1], 3, decoder_channels[1])\n",
    "        self.decode1 = UpsamplingBlock(decoder_channels[1], feature_channels[0], 3, decoder_channels[2])\n",
    "        self.decode0 = OutputBlock(decoder_channels[2], 3, decoder_channels[3])\n",
    "\n",
    "    def forward(self,\n",
    "                s0: Tensor, f1: Tensor, f2: Tensor, f3: Tensor, f4: Tensor,\n",
    "                r1: Optional[Tensor], r2: Optional[Tensor],\n",
    "                r3: Optional[Tensor], r4: Optional[Tensor]):\n",
    "        s1, s2, s3 = self.avgpool(s0)\n",
    "        x4, r4 = self.decode4(f4, r4)\n",
    "        x3, r3 = self.decode3(x4, f3, s3, r3)\n",
    "        x2, r2 = self.decode2(x3, f2, s2, r2)\n",
    "        x1, r1 = self.decode1(x2, f1, s1, r1)\n",
    "        x0 = self.decode0(x1, s0)\n",
    "        return x0, r1, r2, r3, r4\n",
    "    \n",
    "\n",
    "class AvgPool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.avgpool = nn.AvgPool2d(2, 2, count_include_pad=False, ceil_mode=True)\n",
    "        \n",
    "    def forward_single_frame(self, s0):\n",
    "        s1 = self.avgpool(s0)\n",
    "        s2 = self.avgpool(s1)\n",
    "        s3 = self.avgpool(s2)\n",
    "        return s1, s2, s3\n",
    "    \n",
    "    def forward_time_series(self, s0):\n",
    "        B, T = s0.shape[:2]\n",
    "        s0 = s0.flatten(0, 1)\n",
    "        s1, s2, s3 = self.forward_single_frame(s0)\n",
    "        s1 = s1.unflatten(0, (B, T))\n",
    "        s2 = s2.unflatten(0, (B, T))\n",
    "        s3 = s3.unflatten(0, (B, T))\n",
    "        return s1, s2, s3\n",
    "    \n",
    "    def forward(self, s0):\n",
    "        if s0.ndim == 5:\n",
    "            return self.forward_time_series(s0)\n",
    "        else:\n",
    "            return self.forward_single_frame(s0)\n",
    "\n",
    "\n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.gru = ConvGRU(channels // 2)\n",
    "        \n",
    "    def forward(self, x, r: Optional[Tensor]):\n",
    "        a, b = x.split(self.channels // 2, dim=-3)\n",
    "        b, r = self.gru(b, r)\n",
    "        x = torch.cat([a, b], dim=-3)\n",
    "        return x, r\n",
    "\n",
    "    \n",
    "class UpsamplingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, skip_channels, src_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.out_channels = out_channels\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + skip_channels + src_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.gru = ConvGRU(out_channels // 2)\n",
    "\n",
    "    def forward_single_frame(self, x, f, s, r: Optional[Tensor]):\n",
    "        x = self.upsample(x)\n",
    "        x = x[:, :, :s.size(2), :s.size(3)]\n",
    "        x = torch.cat([x, f, s], dim=1)\n",
    "        x = self.conv(x)\n",
    "        a, b = x.split(self.out_channels // 2, dim=1)\n",
    "        b, r = self.gru(b, r)\n",
    "        x = torch.cat([a, b], dim=1)\n",
    "        return x, r\n",
    "    \n",
    "    def forward_time_series(self, x, f, s, r: Optional[Tensor]):\n",
    "        B, T, _, H, W = s.shape\n",
    "        x = x.flatten(0, 1)\n",
    "        f = f.flatten(0, 1)\n",
    "        s = s.flatten(0, 1)\n",
    "        x = self.upsample(x)\n",
    "        x = x[:, :, :H, :W]\n",
    "        x = torch.cat([x, f, s], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = x.unflatten(0, (B, T))\n",
    "        a, b = x.split(self.out_channels // 2, dim=2)\n",
    "        b, r = self.gru(b, r)\n",
    "        x = torch.cat([a, b], dim=2)\n",
    "        return x, r\n",
    "    \n",
    "    def forward(self, x, f, s, r: Optional[Tensor]):\n",
    "        if x.ndim == 5:\n",
    "            return self.forward_time_series(x, f, s, r)\n",
    "        else:\n",
    "            return self.forward_single_frame(x, f, s, r)\n",
    "\n",
    "\n",
    "class OutputBlock(nn.Module):\n",
    "    def __init__(self, in_channels, src_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels + src_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "    def forward_single_frame(self, x, s):\n",
    "        x = self.upsample(x)\n",
    "        x = x[:, :, :s.size(2), :s.size(3)]\n",
    "        x = torch.cat([x, s], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "    def forward_time_series(self, x, s):\n",
    "        B, T, _, H, W = s.shape\n",
    "        x = x.flatten(0, 1)\n",
    "        s = s.flatten(0, 1)\n",
    "        x = self.upsample(x)\n",
    "        x = x[:, :, :H, :W]\n",
    "        x = torch.cat([x, s], dim=1)\n",
    "        x = self.conv(x)\n",
    "        x = x.unflatten(0, (B, T))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, s):\n",
    "        if x.ndim == 5:\n",
    "            return self.forward_time_series(x, s)\n",
    "        else:\n",
    "            return self.forward_single_frame(x, s)\n",
    "\n",
    "\n",
    "class ConvGRU(nn.Module):\n",
    "    def __init__(self,\n",
    "                 channels: int,\n",
    "                 kernel_size: int = 3,\n",
    "                 padding: int = 1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.ih = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels * 2, kernel_size, padding=padding),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.hh = nn.Sequential(\n",
    "            nn.Conv2d(channels * 2, channels, kernel_size, padding=padding),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward_single_frame(self, x, h):\n",
    "        r, z = self.ih(torch.cat([x, h], dim=1)).split(self.channels, dim=1)\n",
    "        c = self.hh(torch.cat([x, r * h], dim=1))\n",
    "        h = (1 - z) * h + z * c\n",
    "        return h, h\n",
    "    \n",
    "    def forward_time_series(self, x, h):\n",
    "        o = []\n",
    "        for xt in x.unbind(dim=1):\n",
    "            ot, h = self.forward_single_frame(xt, h)\n",
    "            o.append(ot)\n",
    "        o = torch.stack(o, dim=1)\n",
    "        return o, h\n",
    "        \n",
    "    def forward(self, x, h: Optional[Tensor]):\n",
    "        if h is None:\n",
    "            h = torch.zeros((x.size(0), x.size(-3), x.size(-2), x.size(-1)),\n",
    "                            device=x.device, dtype=x.dtype)\n",
    "        \n",
    "        if x.ndim == 5:\n",
    "            return self.forward_time_series(x, h)\n",
    "        else:\n",
    "            return self.forward_single_frame(x, h)\n",
    "\n",
    "\n",
    "class Projection(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "    \n",
    "    def forward_single_frame(self, x):\n",
    "        return self.conv(x)\n",
    "    \n",
    "    def forward_time_series(self, x):\n",
    "        B, T = x.shape[:2]\n",
    "        return self.conv(x.flatten(0, 1)).unflatten(0, (B, T))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if x.ndim == 5:\n",
    "            return self.forward_time_series(x)\n",
    "        else:\n",
    "            return self.forward_single_frame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5dabed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNetV3LargeEncoder(MobileNetV3):\n",
    "    def __init__(self, pretrained: bool = False):\n",
    "        super().__init__(\n",
    "            inverted_residual_setting=[\n",
    "                InvertedResidualConfig( 16, 3,  16,  16, False, \"RE\", 1, 1, 1),\n",
    "                InvertedResidualConfig( 16, 3,  64,  24, False, \"RE\", 2, 1, 1),  # C1\n",
    "                InvertedResidualConfig( 24, 3,  72,  24, False, \"RE\", 1, 1, 1),\n",
    "                InvertedResidualConfig( 24, 5,  72,  40,  True, \"RE\", 2, 1, 1),  # C2\n",
    "                InvertedResidualConfig( 40, 5, 120,  40,  True, \"RE\", 1, 1, 1),\n",
    "                InvertedResidualConfig( 40, 5, 120,  40,  True, \"RE\", 1, 1, 1),\n",
    "                InvertedResidualConfig( 40, 3, 240,  80, False, \"HS\", 2, 1, 1),  # C3\n",
    "                InvertedResidualConfig( 80, 3, 200,  80, False, \"HS\", 1, 1, 1),\n",
    "                InvertedResidualConfig( 80, 3, 184,  80, False, \"HS\", 1, 1, 1),\n",
    "                InvertedResidualConfig( 80, 3, 184,  80, False, \"HS\", 1, 1, 1),\n",
    "                InvertedResidualConfig( 80, 3, 480, 112,  True, \"HS\", 1, 1, 1),\n",
    "                InvertedResidualConfig(112, 3, 672, 112,  True, \"HS\", 1, 1, 1),\n",
    "                InvertedResidualConfig(112, 5, 672, 160,  True, \"HS\", 2, 2, 1),  # C4\n",
    "                InvertedResidualConfig(160, 5, 960, 160,  True, \"HS\", 1, 2, 1),\n",
    "                InvertedResidualConfig(160, 5, 960, 160,  True, \"HS\", 1, 2, 1),\n",
    "            ],\n",
    "            last_channel=1280\n",
    "        )\n",
    "        \n",
    "        if pretrained:\n",
    "            self.load_state_dict(torch.hub.load_state_dict_from_url(\n",
    "                'https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth'))\n",
    "\n",
    "        del self.avgpool\n",
    "        del self.classifier\n",
    "        \n",
    "    def forward_single_frame(self, x):\n",
    "        x = normalize(x, [0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        \n",
    "        x = self.features[0](x)\n",
    "        x = self.features[1](x)\n",
    "        f1 = x\n",
    "        x = self.features[2](x)\n",
    "        x = self.features[3](x)\n",
    "        f2 = x\n",
    "        x = self.features[4](x)\n",
    "        x = self.features[5](x)\n",
    "        x = self.features[6](x)\n",
    "        f3 = x\n",
    "        x = self.features[7](x)\n",
    "        x = self.features[8](x)\n",
    "        x = self.features[9](x)\n",
    "        x = self.features[10](x)\n",
    "        x = self.features[11](x)\n",
    "        x = self.features[12](x)\n",
    "        x = self.features[13](x)\n",
    "        x = self.features[14](x)\n",
    "        x = self.features[15](x)\n",
    "        x = self.features[16](x)\n",
    "        f4 = x\n",
    "        return [f1, f2, f3, f4]\n",
    "    \n",
    "    def forward_time_series(self, x):\n",
    "        B, T = x.shape[:2]\n",
    "        features = self.forward_single_frame(x.flatten(0, 1))\n",
    "        features = [f.unflatten(0, (B, T)) for f in features]\n",
    "        return features\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.ndim == 5:\n",
    "            return self.forward_time_series(x)\n",
    "        else:\n",
    "            return self.forward_single_frame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46531fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LRASPP(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.aspp1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.aspp2 = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward_single_frame(self, x):\n",
    "        return self.aspp1(x) * self.aspp2(x)\n",
    "    \n",
    "    def forward_time_series(self, x):\n",
    "        B, T = x.shape[:2]\n",
    "        x = self.forward_single_frame(x.flatten(0, 1)).unflatten(0, (B, T))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if x.ndim == 5:\n",
    "            return self.forward_time_series(x)\n",
    "        else:\n",
    "            return self.forward_single_frame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ade9556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MattingNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 variant: str = 'mobilenetv3',\n",
    "                 refiner: str = 'deep_guided_filter',\n",
    "                 pretrained_backbone: bool = False):\n",
    "        super().__init__()\n",
    "        assert variant in ['mobilenetv3', 'resnet50']\n",
    "        assert refiner in ['fast_guided_filter', 'deep_guided_filter']\n",
    "        \n",
    "\n",
    "        self.backbone = MobileNetV3LargeEncoder(pretrained_backbone)\n",
    "        self.aspp = LRASPP(960, 128)\n",
    "        self.decoder = RecurrentDecoder([16, 24, 40, 128], [80, 40, 32, 16])\n",
    "\n",
    "            \n",
    "        self.project_mat = Projection(16, 4)\n",
    "        self.project_seg = Projection(16, 1)\n",
    "\n",
    "\n",
    "        self.refiner = FastGuidedFilterRefiner()\n",
    "        \n",
    "    def forward(self,\n",
    "                src: Tensor,\n",
    "                r1: Optional[Tensor] = None,\n",
    "                r2: Optional[Tensor] = None,\n",
    "                r3: Optional[Tensor] = None,\n",
    "                r4: Optional[Tensor] = None,\n",
    "                downsample_ratio: float = 1,\n",
    "                segmentation_pass: bool = False):\n",
    "        \n",
    "        if downsample_ratio != 1:\n",
    "            src_sm = self._interpolate(src, scale_factor=downsample_ratio)\n",
    "        else:\n",
    "            src_sm = src\n",
    "        \n",
    "        f1, f2, f3, f4 = self.backbone(src_sm)\n",
    "        f4 = self.aspp(f4)\n",
    "        hid, *rec = self.decoder(src_sm, f1, f2, f3, f4, r1, r2, r3, r4)\n",
    "        \n",
    "        if not segmentation_pass:\n",
    "            fgr_residual, pha = self.project_mat(hid).split([3, 1], dim=-3)\n",
    "            if downsample_ratio != 1:\n",
    "                fgr_residual, pha = self.refiner(src, src_sm, fgr_residual, pha, hid)\n",
    "            fgr = fgr_residual + src\n",
    "            fgr = fgr.clamp(0., 1.)\n",
    "            pha = pha.clamp(0., 1.)\n",
    "            return [fgr, pha, *rec]\n",
    "        else:\n",
    "            seg = self.project_seg(hid)\n",
    "            return [seg, *rec]\n",
    "\n",
    "    def _interpolate(self, x: Tensor, scale_factor: float):\n",
    "        if x.ndim == 5:\n",
    "            B, T = x.shape[:2]\n",
    "            x = F.interpolate(x.flatten(0, 1), scale_factor=scale_factor,\n",
    "                mode='bilinear', align_corners=False, recompute_scale_factor=False)\n",
    "            x = x.unflatten(0, (B, T))\n",
    "        else:\n",
    "            x = F.interpolate(x, scale_factor=scale_factor,\n",
    "                mode='bilinear', align_corners=False, recompute_scale_factor=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "640c99e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MattingNetwork(variant=\"mobilenetv3\",refiner=\"deep_guided_filter\", pretrained_backbone=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83a1010b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save('rvm_model_torchscript.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e11124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, rank, world_size):\n",
    "        self.parse_args()\n",
    "        self.init_distributed(rank, world_size)\n",
    "        self.init_datasets()\n",
    "        self.init_model()\n",
    "        self.init_writer()\n",
    "        self.train()\n",
    "        self.cleanup()\n",
    "        \n",
    "    def parse_args(self):\n",
    "        parser = argparse.ArgumentParser()\n",
    "        # Model\n",
    "        parser.add_argument('--model-variant', type=str, required=True, choices=['mobilenetv3'])\n",
    "        # Matting dataset\n",
    "        parser.add_argument('--dataset', type=str, required=True, choices=['videomatte', 'imagematte'])\n",
    "        # Learning rate\n",
    "        parser.add_argument('--learning-rate-backbone', type=float, required=True)\n",
    "        parser.add_argument('--learning-rate-aspp', type=float, required=True)\n",
    "        parser.add_argument('--learning-rate-decoder', type=float, required=True)\n",
    "        parser.add_argument('--learning-rate-refiner', type=float, required=True)\n",
    "        # Training setting\n",
    "        parser.add_argument('--train-hr', action='store_true')\n",
    "        parser.add_argument('--resolution-lr', type=int, default=512)\n",
    "        parser.add_argument('--resolution-hr', type=int, default=2048)\n",
    "        parser.add_argument('--seq-length-lr', type=int, required=True)\n",
    "        parser.add_argument('--seq-length-hr', type=int, default=6)\n",
    "        parser.add_argument('--downsample-ratio', type=float, default=0.25)\n",
    "        parser.add_argument('--batch-size-per-gpu', type=int, default=1)\n",
    "        parser.add_argument('--num-workers', type=int, default=8)\n",
    "        parser.add_argument('--epoch-start', type=int, default=0)\n",
    "        parser.add_argument('--epoch-end', type=int, default=16)\n",
    "        # Tensorboard logging\n",
    "        parser.add_argument('--log-dir', type=str, required=True)\n",
    "        parser.add_argument('--log-train-loss-interval', type=int, default=20)\n",
    "        parser.add_argument('--log-train-images-interval', type=int, default=500)\n",
    "        # Checkpoint loading and saving\n",
    "        parser.add_argument('--checkpoint', type=str)\n",
    "        parser.add_argument('--checkpoint-dir', type=str, required=True)\n",
    "        parser.add_argument('--checkpoint-save-interval', type=int, default=500)\n",
    "        # Distributed\n",
    "        parser.add_argument('--distributed-addr', type=str, default='localhost')\n",
    "        parser.add_argument('--distributed-port', type=str, default='12355')\n",
    "        # Debugging\n",
    "        parser.add_argument('--disable-progress-bar', action='store_true')\n",
    "        parser.add_argument('--disable-validation', action='store_true')\n",
    "        parser.add_argument('--disable-mixed-precision', action='store_true')\n",
    "        self.args = parser.parse_args()\n",
    "        \n",
    "    def init_distributed(self, rank, world_size):\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.log('Initializing distributed')\n",
    "        os.environ['MASTER_ADDR'] = self.args.distributed_addr\n",
    "        os.environ['MASTER_PORT'] = self.args.distributed_port\n",
    "        dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    \n",
    "    def init_datasets(self):\n",
    "        self.log('Initializing matting datasets')\n",
    "        size_hr = (self.args.resolution_hr, self.args.resolution_hr)\n",
    "        size_lr = (self.args.resolution_lr, self.args.resolution_lr)\n",
    "        \n",
    "        # Matting datasets:\n",
    "        if self.args.dataset == 'videomatte':\n",
    "            self.dataset_lr_train = VideoMatteDataset(\n",
    "                videomatte_dir=DATA_PATHS['videomatte']['train'],\n",
    "                background_image_dir=DATA_PATHS['background_images']['train'],\n",
    "                size=self.args.resolution_lr,\n",
    "                seq_length=self.args.seq_length_lr,\n",
    "                seq_sampler=TrainFrameSampler(),\n",
    "                transform=VideoMatteTrainAugmentation(size_lr))\n",
    "            if self.args.train_hr:\n",
    "                self.dataset_hr_train = VideoMatteDataset(\n",
    "                    videomatte_dir=DATA_PATHS['videomatte']['train'],\n",
    "                    background_image_dir=DATA_PATHS['background_images']['train'],\n",
    "                    size=self.args.resolution_hr,\n",
    "                    seq_length=self.args.seq_length_hr,\n",
    "                    seq_sampler=TrainFrameSampler(),\n",
    "                    transform=VideoMatteTrainAugmentation(size_hr))\n",
    "            self.dataset_valid = VideoMatteDataset(\n",
    "                videomatte_dir=DATA_PATHS['videomatte']['valid'],\n",
    "                background_image_dir=DATA_PATHS['background_images']['valid'],\n",
    "                size=self.args.resolution_hr if self.args.train_hr else self.args.resolution_lr,\n",
    "                seq_length=self.args.seq_length_hr if self.args.train_hr else self.args.seq_length_lr,\n",
    "                seq_sampler=ValidFrameSampler(),\n",
    "                transform=VideoMatteValidAugmentation(size_hr if self.args.train_hr else size_lr))\n",
    "        \n",
    "            \n",
    "        # Matting dataloaders:\n",
    "        self.datasampler_lr_train = DistributedSampler(\n",
    "            dataset=self.dataset_lr_train,\n",
    "            rank=self.rank,\n",
    "            num_replicas=self.world_size,\n",
    "            shuffle=True)\n",
    "        self.dataloader_lr_train = DataLoader(\n",
    "            dataset=self.dataset_lr_train,\n",
    "            batch_size=self.args.batch_size_per_gpu,\n",
    "            num_workers=self.args.num_workers,\n",
    "            sampler=self.datasampler_lr_train,\n",
    "            pin_memory=True)\n",
    "        if self.args.train_hr:\n",
    "            self.datasampler_hr_train = DistributedSampler(\n",
    "                dataset=self.dataset_hr_train,\n",
    "                rank=self.rank,\n",
    "                num_replicas=self.world_size,\n",
    "                shuffle=True)\n",
    "            self.dataloader_hr_train = DataLoader(\n",
    "                dataset=self.dataset_hr_train,\n",
    "                batch_size=self.args.batch_size_per_gpu,\n",
    "                num_workers=self.args.num_workers,\n",
    "                sampler=self.datasampler_hr_train,\n",
    "                pin_memory=True)\n",
    "        self.dataloader_valid = DataLoader(\n",
    "            dataset=self.dataset_valid,\n",
    "            batch_size=self.args.batch_size_per_gpu,\n",
    "            num_workers=self.args.num_workers,\n",
    "            pin_memory=True)\n",
    "        \n",
    "        # Segementation datasets\n",
    "        '''\n",
    "        self.log('Initializing image segmentation datasets')\n",
    "        self.dataset_seg_image = ConcatDataset([\n",
    "            CocoPanopticDataset(\n",
    "                imgdir=DATA_PATHS['coco_panoptic']['imgdir'],\n",
    "                anndir=DATA_PATHS['coco_panoptic']['anndir'],\n",
    "                annfile=DATA_PATHS['coco_panoptic']['annfile'],\n",
    "                transform=CocoPanopticTrainAugmentation(size_lr)),\n",
    "            SuperviselyPersonDataset(\n",
    "                imgdir=DATA_PATHS['spd']['imgdir'],\n",
    "                segdir=DATA_PATHS['spd']['segdir'],\n",
    "                transform=CocoPanopticTrainAugmentation(size_lr))\n",
    "        ])\n",
    "        \n",
    "        self.datasampler_seg_image = DistributedSampler(\n",
    "            dataset=self.dataset_seg_image,\n",
    "            rank=self.rank,\n",
    "            num_replicas=self.world_size,\n",
    "            shuffle=True)\n",
    "        self.dataloader_seg_image = DataLoader(\n",
    "            dataset=self.dataset_seg_image,\n",
    "            batch_size=self.args.batch_size_per_gpu * self.args.seq_length_lr,\n",
    "            num_workers=self.args.num_workers,\n",
    "            sampler=self.datasampler_seg_image,\n",
    "            pin_memory=True)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "    def init_model(self):\n",
    "        self.log('Initializing model')\n",
    "        self.model = MattingNetwork(self.args.model_variant, pretrained_backbone=True).to(self.rank)\n",
    "        \n",
    "        if self.args.checkpoint:\n",
    "            self.log(f'Restoring from checkpoint: {self.args.checkpoint}')\n",
    "            self.log(self.model.load_state_dict(\n",
    "                torch.load(self.args.checkpoint, map_location=f'cuda:{self.rank}')))\n",
    "            \n",
    "        self.model = nn.SyncBatchNorm.convert_sync_batchnorm(self.model)\n",
    "        self.model_ddp = DDP(self.model, device_ids=[self.rank], broadcast_buffers=False, find_unused_parameters=True)\n",
    "        self.optimizer = Adam([\n",
    "            {'params': self.model.backbone.parameters(), 'lr': self.args.learning_rate_backbone},\n",
    "            {'params': self.model.aspp.parameters(), 'lr': self.args.learning_rate_aspp},\n",
    "            {'params': self.model.decoder.parameters(), 'lr': self.args.learning_rate_decoder},\n",
    "            {'params': self.model.project_mat.parameters(), 'lr': self.args.learning_rate_decoder},\n",
    "            {'params': self.model.project_seg.parameters(), 'lr': self.args.learning_rate_decoder},\n",
    "            {'params': self.model.refiner.parameters(), 'lr': self.args.learning_rate_refiner},\n",
    "        ])\n",
    "        self.scaler = GradScaler()\n",
    "        \n",
    "    def init_writer(self):\n",
    "        if self.rank == 0:\n",
    "            self.log('Initializing writer')\n",
    "            self.writer = SummaryWriter(self.args.log_dir)\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.args.epoch_start, self.args.epoch_end):\n",
    "            self.epoch = epoch\n",
    "            self.step = epoch * len(self.dataloader_lr_train)\n",
    "            \n",
    "            if not self.args.disable_validation:\n",
    "                self.validate()\n",
    "            \n",
    "            self.log(f'Training epoch: {epoch}')\n",
    "            for true_fgr, true_pha, true_bgr in tqdm(self.dataloader_lr_train, disable=self.args.disable_progress_bar, dynamic_ncols=True):\n",
    "                # Low resolution pass\n",
    "                self.train_mat(true_fgr, true_pha, true_bgr, downsample_ratio=1, tag='lr')\n",
    "\n",
    "                # High resolution pass\n",
    "                if self.args.train_hr:\n",
    "                    true_fgr, true_pha, true_bgr = self.load_next_mat_hr_sample()\n",
    "                    self.train_mat(true_fgr, true_pha, true_bgr, downsample_ratio=self.args.downsample_ratio, tag='hr')\n",
    "                '''\n",
    "                # Segmentation pass\n",
    "                if self.step % 2 == 0:\n",
    "                    true_img, true_seg = self.load_next_seg_video_sample()\n",
    "                    self.train_seg(true_img, true_seg, log_label='seg_video')\n",
    "                else:\n",
    "                    true_img, true_seg = self.load_next_seg_image_sample()\n",
    "                    self.train_seg(true_img.unsqueeze(1), true_seg.unsqueeze(1), log_label='seg_image')\n",
    "                '''\n",
    "                if self.step % self.args.checkpoint_save_interval == 0:\n",
    "                    self.save()\n",
    "                    \n",
    "                self.step += 1\n",
    "                \n",
    "    def train_mat(self, true_fgr, true_pha, true_bgr, downsample_ratio, tag):\n",
    "        true_fgr = true_fgr.to(self.rank, non_blocking=True)\n",
    "        true_pha = true_pha.to(self.rank, non_blocking=True)\n",
    "        true_bgr = true_bgr.to(self.rank, non_blocking=True)\n",
    "        true_fgr, true_pha, true_bgr = self.random_crop(true_fgr, true_pha, true_bgr)\n",
    "        true_src = true_fgr * true_pha + true_bgr * (1 - true_pha)\n",
    "        \n",
    "        with autocast(enabled=not self.args.disable_mixed_precision):\n",
    "            pred_fgr, pred_pha = self.model_ddp(true_src, downsample_ratio=downsample_ratio)[:2]\n",
    "            loss = matting_loss(pred_fgr, pred_pha, true_fgr, true_pha)\n",
    "\n",
    "        self.scaler.scale(loss['total']).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        if self.rank == 0 and self.step % self.args.log_train_loss_interval == 0:\n",
    "            for loss_name, loss_value in loss.items():\n",
    "                self.writer.add_scalar(f'train_{tag}_{loss_name}', loss_value, self.step)\n",
    "            \n",
    "        if self.rank == 0 and self.step % self.args.log_train_images_interval == 0:\n",
    "            self.writer.add_image(f'train_{tag}_pred_fgr', make_grid(pred_fgr.flatten(0, 1), nrow=pred_fgr.size(1)), self.step)\n",
    "            self.writer.add_image(f'train_{tag}_pred_pha', make_grid(pred_pha.flatten(0, 1), nrow=pred_pha.size(1)), self.step)\n",
    "            self.writer.add_image(f'train_{tag}_true_fgr', make_grid(true_fgr.flatten(0, 1), nrow=true_fgr.size(1)), self.step)\n",
    "            self.writer.add_image(f'train_{tag}_true_pha', make_grid(true_pha.flatten(0, 1), nrow=true_pha.size(1)), self.step)\n",
    "            self.writer.add_image(f'train_{tag}_true_src', make_grid(true_src.flatten(0, 1), nrow=true_src.size(1)), self.step)\n",
    "            \n",
    "    def train_seg(self, true_img, true_seg, log_label):\n",
    "        true_img = true_img.to(self.rank, non_blocking=True)\n",
    "        true_seg = true_seg.to(self.rank, non_blocking=True)\n",
    "        \n",
    "        true_img, true_seg = self.random_crop(true_img, true_seg)\n",
    "        \n",
    "        with autocast(enabled=not self.args.disable_mixed_precision):\n",
    "            pred_seg = self.model_ddp(true_img, segmentation_pass=True)[0]\n",
    "            loss = segmentation_loss(pred_seg, true_seg)\n",
    "        \n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        if self.rank == 0 and (self.step - self.step % 2) % self.args.log_train_loss_interval == 0:\n",
    "            self.writer.add_scalar(f'{log_label}_loss', loss, self.step)\n",
    "        \n",
    "        if self.rank == 0 and (self.step - self.step % 2) % self.args.log_train_images_interval == 0:\n",
    "            self.writer.add_image(f'{log_label}_pred_seg', make_grid(pred_seg.flatten(0, 1).float().sigmoid(), nrow=self.args.seq_length_lr), self.step)\n",
    "            self.writer.add_image(f'{log_label}_true_seg', make_grid(true_seg.flatten(0, 1), nrow=self.args.seq_length_lr), self.step)\n",
    "            self.writer.add_image(f'{log_label}_true_img', make_grid(true_img.flatten(0, 1), nrow=self.args.seq_length_lr), self.step)\n",
    "    \n",
    "    def load_next_mat_hr_sample(self):\n",
    "        try:\n",
    "            sample = next(self.dataiterator_mat_hr)\n",
    "        except:\n",
    "            self.datasampler_hr_train.set_epoch(self.datasampler_hr_train.epoch + 1)\n",
    "            self.dataiterator_mat_hr = iter(self.dataloader_hr_train)\n",
    "            sample = next(self.dataiterator_mat_hr)\n",
    "        return sample\n",
    "    \n",
    "    def load_next_seg_video_sample(self):\n",
    "        try:\n",
    "            sample = next(self.dataiterator_seg_video)\n",
    "        except:\n",
    "            self.datasampler_seg_video.set_epoch(self.datasampler_seg_video.epoch + 1)\n",
    "            self.dataiterator_seg_video = iter(self.dataloader_seg_video)\n",
    "            sample = next(self.dataiterator_seg_video)\n",
    "        return sample\n",
    "    \n",
    "    def load_next_seg_image_sample(self):\n",
    "        try:\n",
    "            sample = next(self.dataiterator_seg_image)\n",
    "        except:\n",
    "            self.datasampler_seg_image.set_epoch(self.datasampler_seg_image.epoch + 1)\n",
    "            self.dataiterator_seg_image = iter(self.dataloader_seg_image)\n",
    "            sample = next(self.dataiterator_seg_image)\n",
    "        return sample\n",
    "    \n",
    "    def validate(self):\n",
    "        if self.rank == 0:\n",
    "            self.log(f'Validating at the start of epoch: {self.epoch}')\n",
    "            self.model_ddp.eval()\n",
    "            total_loss, total_count = 0, 0\n",
    "            with torch.no_grad():\n",
    "                with autocast(enabled=not self.args.disable_mixed_precision):\n",
    "                    for true_fgr, true_pha, true_bgr in tqdm(self.dataloader_valid, disable=self.args.disable_progress_bar, dynamic_ncols=True):\n",
    "                        true_fgr = true_fgr.to(self.rank, non_blocking=True)\n",
    "                        true_pha = true_pha.to(self.rank, non_blocking=True)\n",
    "                        true_bgr = true_bgr.to(self.rank, non_blocking=True)\n",
    "                        true_src = true_fgr * true_pha + true_bgr * (1 - true_pha)\n",
    "                        batch_size = true_src.size(0)\n",
    "                        pred_fgr, pred_pha = self.model(true_src)[:2]\n",
    "                        total_loss += matting_loss(pred_fgr, pred_pha, true_fgr, true_pha)['total'].item() * batch_size\n",
    "                        total_count += batch_size\n",
    "            avg_loss = total_loss / total_count\n",
    "            self.log(f'Validation set average loss: {avg_loss}')\n",
    "            self.writer.add_scalar('valid_loss', avg_loss, self.step)\n",
    "            self.model_ddp.train()\n",
    "        dist.barrier()\n",
    "    \n",
    "    def random_crop(self, *imgs):\n",
    "        h, w = imgs[0].shape[-2:]\n",
    "        w = random.choice(range(w // 2, w))\n",
    "        h = random.choice(range(h // 2, h))\n",
    "        results = []\n",
    "        for img in imgs:\n",
    "            B, T = img.shape[:2]\n",
    "            img = img.flatten(0, 1)\n",
    "            img = F.interpolate(img, (max(h, w), max(h, w)), mode='bilinear', align_corners=False)\n",
    "            img = center_crop(img, (h, w))\n",
    "            img = img.reshape(B, T, *img.shape[1:])\n",
    "            results.append(img)\n",
    "        return results\n",
    "    \n",
    "    def save(self):\n",
    "        if self.rank == 0:\n",
    "            os.makedirs(self.args.checkpoint_dir, exist_ok=True)\n",
    "            torch.save(self.model.state_dict(), os.path.join(self.args.checkpoint_dir, f'epoch-{self.epoch}.pth'))\n",
    "            self.log('Model saved')\n",
    "        dist.barrier()\n",
    "        \n",
    "    def cleanup(self):\n",
    "        dist.destroy_process_group()\n",
    "        \n",
    "    def log(self, msg):\n",
    "        print(f'[GPU{self.rank}] {msg}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9651a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.argv=[\n",
    "    '--model-variant', 'mobilenetv3',\n",
    "    '--dataset', 'videomatte',\n",
    "    '--resolution-lr', '512',\n",
    "    '--seq-length-lr', '15',\n",
    "    '--learning-rate-backbone', '0.0001',\n",
    "    '--learning-rate-aspp', '0.0002',\n",
    "    '--learning-rate-decoder', '0.0002',\n",
    "    '--learning-rate-refiner', '0',\n",
    "    '--checkpoint-dir', 'checkpoint/stage1',\n",
    "    '--log-dir', 'log/stage1',\n",
    "    '--epoch-start', '0',\n",
    "    '--epoch-end', '5']\n",
    "\n",
    "world_size = torch.cuda.device_count()\n",
    "mp.spawn(\n",
    "    Trainer,\n",
    "    nprocs=world_size,\n",
    "    args=(world_size,),\n",
    "    join=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58f71b24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "яз зы ык язы зык\n"
     ]
    }
   ],
   "source": [
    "def n_grams(word, n):\n",
    "    return [word[i:i+n] for i in range(len(word) - n + 1)]\n",
    "word = \"язык\"\n",
    "bigrams = n_grams(word, 2)\n",
    "trigrams = n_grams(word, 3)\n",
    "# Объединяем результаты и выводим\n",
    "result = bigrams + trigrams\n",
    "print(\" \".join(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
